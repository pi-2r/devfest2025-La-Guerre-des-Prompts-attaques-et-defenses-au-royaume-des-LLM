# Cadres de S√©curit√© R√©f√©rents

[<img src="img/step4.jpg" alt="Arwen" >](https://www.youtube.com/watch?v=fd2AO0gr3Rc)
> "if you want him come and claim him", Arwen, LOTR - The Followship of the Ring

## üéØ Objectifs de cette √©tape
- Comprendre les cadres de s√©curit√© existants pour les LLM (mod√®les de langage de grande taille)
- Identifier les principaux risques et vuln√©rabilit√©s li√©s √† l‚Äôutilisation des LLM
- D√©couvrir les frameworks et r√©f√©rentiels de s√©curit√© d√©di√©s (OWASP Top 10 LLM, SAIF, MITRE ATLAS)
- Appr√©hender les r√©glementations l√©gislatives encadrant les LLM (√âtats-Unis, Union europ√©enne)
- Acc√©der √† des ressources pour approfondir la s√©curisation des applications IA


## Sommaire

- [OWASP Top 10 for LLM Applications](#owasp-top-10-for-llm-applications)
  - [Plus en d√©tail](#plus-en-d√©tail)


- [Soyez SAIF avec le Secure AI Framework](#soyez-saif-avec-le-secure-ai-framework)
  - [Cadre Secure AI Framework (SAIF) de Google](#cadre-secure-ai-framework-saif-de-google)
  - [Les quatre grandes cat√©gories du SAIF](#les-quatre-grandes-cat√©gories-du-saif)
  - [Les six √©l√©ments fondamentaux du SAIF](#les-six-√©l√©ments-fondamentaux-du-saif)
  - [Cartographie des risques et contr√¥les SAIF](#cartographie-des-risques-et-contr√¥les-saif)
  - [Mise en ≈ìuvre et communaut√© SAIF](#mise-en-≈ìuvre-et-communaut√©-saif)


- [MITRE ATLAS, le fil d'Ariane des techniques d'attaque sur l'IA](#mitre-atlas-le-fil-dariane-des-techniques-dattaque-sur-lia)
  - [Objectif du MITRE ATLAS](#objectif-du-mitre-atlas)
  - [Cadre de r√©f√©rence](#cadre-de-r√©f√©rence)
  - [√âl√©ments fondamentaux du MITRE ATLAS](#√©l√©ments-fondamentaux-du-mitre-atlas)
  - [Comment l'utiliser](#comment-lutiliser)


- [R√©glementation l√©gislative des LLM](#r√©glementation-l√©gislative-des-llm)
    - [Enjeux et principes](#enjeux-et-principes)
      - [√âtats-Unis: une r√©gulation sectorielle et centr√©e sur la libert√© d‚Äôexpression](#√©tats-unis-une-r√©gulation-sectorielle-et-centr√©e-sur-la-libert√©-dexpression)
      - [Union europ√©enne: un encadrement structur√© et fond√© sur les risques](#union-europ√©enne-un-encadrement-structur√©-et-fond√©-sur-les-risques)
        - [Digital Services Act (DSA)](#digital-services-act-dsa)
        - [AI Act](#ai-act)
    - [Points de convergence et divergences](#points-de-convergence-et-divergences)


- [√âtape suivante](#√©tape-suivante)
- [Ressources](#ressources)

## OWASP Top 10 for LLM Applications

<a href="https://genai.owasp.org/2023/10/18/llm-to-10-v1-1/" target="_blank">
  <img src="https://genai.owasp.org/wp-content/uploads/2024/05/1697599021768.jpeg" alt="image" width="950" style="transition:0.3s;">
</a>

<a href="https://genai.owasp.org/2023/10/18/llm-to-10-v1-1/" target="_blank"><em>source: genai.owasp.org</em></a>


L‚Äô**OWASP Top 10 for Large Language Model Applications** est aujourd‚Äôhui l‚Äôoutil de r√©f√©rence pour recenser, analyser et 
att√©nuer les principaux risques de s√©curit√© propres √† l‚Äôutilisation des grands mod√®les de langage.


Cette liste √©labor√©e collectivement constitue un guide incontournable pour les d√©veloppeurs, architectes et responsables 
de la s√©curit√© d√©sireux d‚Äôint√©grer l‚ÄôIA g√©n√©rative de mani√®re fiable et s√©curis√©e dans leurs syst√®mes d‚Äôinformation. 
Ce classement a vu le jour gr√¢ce √† l‚Äôengagement de [John Sotiropoulos](https://www.linkedin.com/in/jsotiropoulos/), 
co-pilote du projet, et d‚Äô[Ads Dawson](https://www.linkedin.com/in/adamdawson0/), responsable technique en charge de la 
coordination de la r√©daction des aspects techniques du r√©f√©rentiel.

Voici une synth√®se de l'OWASP Top 10 for LLM Applications 2025 :

| IDENTIFIANT | Description                                                                                                                                                                                                                    |
|-------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **LLM01**   | **Injection de prompt** : Les attaquants manipulent l'entr√©e du LLM directement ou indirectement pour provoquer un comportement malveillant ou ill√©gal.                                                                        |
| **LLM02**   | **Divulgation d‚Äôinformations sensibles** : Les attaquants trompent le LLM pour qu'il r√©v√®le des informations sensibles dans sa r√©ponse.                                                                                        |
| **LLM03**   | **Vuln√©rabilit√©s de la cha√Æne d'approvisionnement** : Les attaquants exploitent des vuln√©rabilit√©s dans n‚Äôimporte quelle partie de la cha√Æne d‚Äôapprovisionnement du LLM.                                                       |
| **LLM04**   | **Empoisonnement des donn√©es et du mod√®le** : Les attaquants injectent des donn√©es malveillantes ou trompeuses dans les donn√©es d'entra√Ænement du LLM, compromettant ses performances ou cr√©ant des portes d√©rob√©es.           |
| **LLM05**   | **Gestion non s√©curis√©e de la sortie** : La sortie du LLM est g√©r√©e de mani√®re non s√©curis√©e, entra√Ænant des vuln√©rabilit√©s d'injection telles que le Cross-Site Scripting (XSS), l'injection SQL ou l'injection de commandes. |
| **LLM06**   | **Acc√®s excessif (agency)** : Les attaquants exploitent l‚Äôacc√®s insuffisamment restreint du LLM √† des syst√®mes ou √† des actions sensibles.                                                                                     |
| **LLM07**   | **Fuite du prompt syst√®me** : Les informations sensibles contenues dans les prompts syst√®me sont accidentellement divulgu√©es.                                                                                                  |
| **LLM08**   | **Vuln√©rabilit√©s sur les vecteurs et embeddings** : Exploitation des faiblesses dans la gestion des vecteurs et embeddings en syst√®mes RAG, causant fuite ou alt√©ration de donn√©es.                                            |
| **LLM09**   | **D√©sinformation** : Le LLM g√©n√®re des informations fausses ou trompeuses, provoquant des risques de s√©curit√© et r√©putationnels.                                                                                               |
| **LLM10**   | **Consommation illimit√©e (Unbounded Consumption)** : Des attaques exploitent la consommation excessive de ressources, entra√Ænant d√©ni de service ou co√ªts financiers.                                                          |


### Plus en d√©tail

<details>
  <summary>Injection de prompt (LLM01)</summary>

L‚Äôinjection de prompt est une vuln√©rabilit√© de s√©curit√© qui survient lorsque qu‚Äôun utilisateur malintentionn√© parvient √† 
manipuler les instructions fournies en entr√©e √† un mod√®le de langage (LLM), l‚Äôamenant ainsi √† adopter un comportement 
impr√©vu ou non d√©sir√©.


Si certaines manipulations peuvent para√Ætre anodines ‚Äî par exemple, d√©tourner un chatbot d‚Äôassistance technique pour lui 
faire donner des recettes de cuisine ‚Äî d‚Äôautres peuvent avoir des cons√©quences bien plus graves. Cette technique peut 
√™tre exploit√©e pour inciter le LLM √† produire de fausses informations, des discours haineux ou encore du contenu 
nuisible, voire ill√©gal.


Dans certains cas, l‚Äôinjection de prompt peut √©galement permettre √† un attaquant d‚Äôextraire des donn√©es sensibles 
pr√©c√©demment communiqu√©es au mod√®le, compromettant ainsi la confidentialit√© des informations trait√©es.
</details>

<details>
  <summary>Divulgation d‚Äôinformations sensibles (LLM02)</summary>

Les mod√®les de langage (LLM) peuvent, de mani√®re involontaire, divulguer des donn√©es confidentielles dans leurs r√©ponses.
Une telle exposition peut entra√Æner un acc√®s non autoris√© √† des informations sensibles, des atteintes √† la vie priv√©e,
voire des failles de s√©curit√©. Il est donc essentiel de restreindre strictement l'acc√®s aux informations que le LLM est
autoris√© √† consulter.


Cela est particuli√®rement important lorsque le mod√®le est utilis√© pour traiter des donn√©es sensibles ou strat√©giques,
comme des informations clients. Dans ces cas, les requ√™tes adress√©es au LLM doivent faire l‚Äôobjet de contr√¥les d‚Äôacc√®s
rigoureux afin de limiter le risque de fuite de donn√©es.

Si le LLM a √©t√© entra√Æn√© ou affin√© √† l‚Äôaide d‚Äôun jeu de donn√©es personnalis√©, il est crucial de garder √† l‚Äôesprit qu‚Äôil
peut √™tre manipul√© (par des attaques d'injection de prompt, par exemple) pour r√©v√©ler des √©l√©ments de ces donn√©es
d‚Äôapprentissage. Ainsi, toute information sensible int√©gr√©e au corpus d‚Äôentra√Ænement doit √™tre soigneusement identifi√©e,
√©valu√©e en fonction de sa criticit√©, et prot√©g√©e en cons√©quence.

De plus, les donn√©es sensibles fournies au LLM via des "prompts" utilisateurs peuvent √™tre expos√©es par des attaques
d‚Äôinjection (voir LLM01), m√™me si l‚Äôon a explicitement demand√© au mod√®le de garder ces informations confidentielles.
Cela souligne la n√©cessit√© de mettre en place des mesures de s√©curit√© adapt√©es √† chaque point de contact entre
l‚Äôutilisateur et le mod√®le.
</details>

<br/>
<br/>
<details>
  <summary>Vuln√©rabilit√©s de la cha√Æne d'approvisionnement (LLM03)</summary>

Les vuln√©rabilit√©s de la cha√Æne d‚Äôapprovisionnement dans le contexte des LLM concernent tous les √©l√©ments impliqu√©s dans
leur d√©veloppement ou leur d√©ploiement. Cela inclut notamment les jeux de donn√©es utilis√©s pour l‚Äôentra√Ænement (voir LLM03),
les mod√®les pr√©entra√Æn√©s fournis par des tiers, ainsi que les plugins, extensions ou autres syst√®mes interagissant
avec le LLM (cf. LLM07).


L‚Äôimpact de ces vuln√©rabilit√©s peut varier consid√©rablement, allant de simples dysfonctionnements √† des cons√©quences
critiques. L‚Äôun des sc√©narios les plus courants est la fuite de donn√©es sensibles ou la divulgation de propri√©t√©
intellectuelle, compromettant la confidentialit√© ou les actifs strat√©giques de l‚Äôorganisation.
</details>
<details>
  <summary>Empoisonnement des donn√©es et du mod√®le (LLM04)</summary>

La qualit√© et les performances d‚Äôun mod√®le de langage (LLM) d√©pendent en grande partie des donn√©es utilis√©es durant sa
phase d'entra√Ænement. L‚Äôempoisonnement des donn√©es d‚Äôentra√Ænement (Training Data Poisoning) consiste √† manipuler tout
ou partie de ces donn√©es afin d‚Äôintroduire des biais volontaires, incitant le mod√®le √† produire des r√©sultats incorrects
ou malveillants.


Selon l‚Äôusage du LLM ainsi compromis, les cons√©quences peuvent aller d‚Äôune perte de cr√©dibilit√© √† des vuln√©rabilit√©s
critiques en mati√®re de s√©curit√©, notamment si le mod√®le g√©n√®re du code r√©utilis√© dans d‚Äôautres composants logiciels.


Pour r√©ussir une attaque par empoisonnement des donn√©es d‚Äôapprentissage, un attaquant doit d‚Äôabord avoir acc√®s au corpus
de donn√©es utilis√© pour entra√Æner le mod√®le. Lorsque l‚Äôentra√Ænement repose sur des donn√©es accessibles publiquement
(comme du contenu web), il est crucial de les nettoyer et de v√©rifier leur int√©grit√© afin d‚Äô√©carter toute source de
biais ou de contenu manipul√©.

Parmi les strat√©gies de mitigation √† adopter, on peut citer :
- La v√©rification fine et r√©guli√®re de la cha√Æne d‚Äôapprovisionnement des donn√©es d‚Äôentra√Ænement
- L‚Äô√©valuation de la l√©gitimit√© et de la provenance des sources
- L‚Äôimpl√©mentation de filtres capables d‚Äôidentifier et d‚Äôexclure les donn√©es incorrectes ou malveillantes

En somme, une attention rigoureuse port√©e √† la qualit√© des donn√©es d‚Äôentra√Ænement est essentielle pour garantir un
comportement fiable et √©thique des LLM.
</details>
<br/>
<br/>
<details>
<summary>Gestion non s√©curis√©e de la sortie (LLM05)</summary>
La gestion non s√©curis√©e de la sortie fait r√©f√©rence √† l'absence de validation, de nettoyage et de contr√¥le appropri√©s 
des r√©ponses g√©n√©r√©es par le LLM avant leur transmission aux autres syst√®mes ou √† l‚Äôutilisateur final. Comme les sorties
du mod√®le peuvent √™tre influenc√©es par des entr√©es malveillantes, cela revient √† accorder un acc√®s indirect √† des 
fonctionnalit√©s suppl√©mentaires, pouvant engendrer des vuln√©rabilit√©s graves.


Cette mauvaise gestion expose le syst√®me √† des attaques telles que le Cross-Site Scripting (XSS), Cross-Site Request 
Forgery (CSRF), Server Side Request Forgery (SSRF), l'√©l√©vation de privil√®ges, ou m√™me l‚Äôex√©cution de code √† distance 
(RCE) sur les syst√®mes backend qui traitent les sorties.


Les causes courantes incluent un manque d‚Äôencodage appropri√© des sorties, une absence de filtrage adapt√© selon le 
contexte (HTML, SQL, commandes syst√®me), et une surveillance limit√©e des comportements anormaux des sorties.


Pour pr√©venir ces failles, il est recommand√© de :

- Traiter les sorties du LLM comme provenant d‚Äôun utilisateur non fiable (mod√®le "zero-trust").

- Valider et assainir rigoureusement toute sortie avant utilisation ou affichage.

- Appliquer un encodage contextuel sp√©cifique (HTML, JavaScript, SQL, etc.).

- Utiliser des politiques de s√©curit√© strictes, comme les Content Security Policies (CSP) pour le web.

- Introduire des m√©canismes de surveillance et d‚Äôalerte en cas de sorties suspectes ou anormales.

En r√©sum√©, la gestion non s√©curis√©e de la sortie est une vuln√©rabilit√© critique qui peut compromettre la s√©curit√© 
globale de l‚Äôapplication en autorisant des ex√©cutions non d√©sir√©es et des attaques via les r√©ponses du LLM.
</details>
<details> 
<summary>Acc√®s excessif (agency) LLM06</summary>
L‚Äôacc√®s excessif (agency) d√©signe une vuln√©rabilit√© majeure dans les applications utilisant des grands mod√®les de 
langage (LLM), o√π le mod√®le ou l‚Äôagent LLM dispose de privil√®ges ou d‚Äôautorisations trop larges, lui permettant 
d‚Äôinteragir avec des syst√®mes, bases de donn√©es ou fonctions sensibles au-del√† de ce qui est strictement n√©cessaire.


Cette surexposition peut permettre √† un attaquant, en manipulant le mod√®le via des prompts ou requ√™tes malveillantes, 
d‚Äôex√©cuter des actions non autoris√©es, telles que modifier, voler ou supprimer des donn√©es, d√©clencher des op√©rations 
critiques, ou √©tendre son contr√¥le dans l‚Äôenvironnement cible.


Les causes typiques incluent une mauvaise gestion des permissions, un cloisonnement insuffisant entre les fonctions 
automatis√©es, ou une architecture d‚Äôagent trop permissive, par exemple avec des LLM agissant comme agents autonomes 
capables d'ex√©cuter des commandes syst√®me sans surveillance.

Pour att√©nuer ce risque, il est recommand√© de :

- Appliquer le principe du moindre privil√®ge : limiter strictement les acc√®s et capacit√©s du LLM √† ce qui est indispensable.

- Mettre en place des contr√¥les d‚Äôacc√®s granulaires et v√©rifier chaque demande du LLM avant ex√©cution.

- Employer des m√©canismes manuels ou automatiques de validation (human-in-the-loop) pour toute op√©ration sensible.

- Segmentation fonctionnelle et strict cloisonnement des agents LLM quand plusieurs sont utilis√©s.

- Surveiller activement les interactions et d√©tecter toute activit√© anormale ou suspecte.

Pour faire simple, l‚Äôacc√®s excessif est un vecteur critique d‚Äôattaque rendant un LLM potentiellement capable de causer 
des dommages importants, et n√©cessite une gouvernance forte et une conception s√©curis√©e d√®s la phase de d√©veloppement.
</details>
<br/>
<br/>

<details>
<summary>Fuite du prompt syst√®me (LLM07)</summary>
La fuite du prompt syst√®me d√©signe la vuln√©rabilit√© o√π les instructions internes ou prompts syst√®me utilis√©s pour 
guider le comportement d'un grand mod√®le de langage (LLM) sont accidentellement expos√©s ou divulgu√©s √† des utilisateurs 
non autoris√©s. Ces prompts syst√®me contiennent souvent des informations sensibles telles que des cl√©s d‚Äôacc√®s, des 
param√®tres de s√©curit√©, des r√®gles m√©tier, ou des contr√¥les de filtrage, qui ne devaient pas √™tre visibles.


La vraie vuln√©rabilit√© ne r√©side pas tant dans la divulgation en soi, mais dans le fait que ces informations sont 
utilis√©es pour d√©l√©guer des contr√¥les d‚Äôacc√®s, de privil√®ges, ou de s√©curit√© au mod√®le lui-m√™me. Une fuite permet donc 
aux attaquants de contourner ces contr√¥les et de r√©aliser des actions non autoris√©es en manipulant le mod√®le.


Les mesures pr√©ventives recommand√©es incluent :

- S√©parer les informations sensibles des prompts syst√®me, en les stockant dans des environnements s√©curis√©s ind√©pendants, inaccessibles directement au LLM.

- Ne pas s‚Äôappuyer sur les prompts syst√®me comme unique m√©canisme de s√©curit√© ; mettre en ≈ìuvre des contr√¥les externes, comme des garde-fous qui inspectent les sorties du mod√®le.

- Appliquer strictement le principe de moindre privil√®ge dans la configuration des agents ou syst√®mes int√©gr√©s aux LLM.

- Utiliser plusieurs agents LLM distincts, chacun avec des acc√®s adapt√©s et limit√©s √† leurs t√¢ches pour minimiser les risques.

Pour faire simple, la fuite du prompt syst√®me est une vuln√©rabilit√© critique car elle compromet les m√©canismes de 
s√©curit√© fondamentaux, exposant les applications LLM √† des attaques √©tendues comme le jailbreak, la divulgation de 
donn√©es, ou des actions malveillantes.
</details>
<details>
<summary>Vuln√©rabilit√©s sur les vecteurs et embeddings (LLM08)</summary>
Les vuln√©rabilit√©s sur les vecteurs et embeddings (LLM08) concernent les failles de s√©curit√© li√©es √† la mani√®re dont 
les LLM traitent, stockent et utilisent les repr√©sentations num√©riques (vecteurs, embeddings) des donn√©es. Ces vecteurs 
permettent au mod√®le de retrouver rapidement des informations et de fournir des r√©ponses contextuelles, notamment dans 
les syst√®mes de g√©n√©ration augment√©e par r√©cup√©ration (RAG).


Les risques majeurs incluent l'acc√®s non autoris√© √† des donn√©es sensibles contenues dans les vecteurs, la fuite 
d'informations entre diff√©rents utilisateurs ou contextes (dans un environnement multi-tenant), ainsi que les attaques 
d'inversion d'embeddings permettant de reconstituer des donn√©es originales √† partir des vecteurs. De plus, le poisoning 
(empoisonnement) des embeddings peut manipuler les sorties du mod√®le, affectant la fiabilit√© et la s√©curit√© des r√©ponses.


Pour r√©duire ces risques, il est essentiel d‚Äôappliquer des contr√¥les rigoureux d‚Äôacc√®s et d‚Äôauthentification aux bases 
de vecteurs, de valider la source et l‚Äôint√©grit√© des donn√©es ins√©r√©es, de classifier et s√©parer les donn√©es selon les 
p√©rim√®tres d‚Äôacc√®s, et de surveiller en continu les activit√©s de r√©cup√©ration pour d√©tecter toute anomalie.


Ces vuln√©rabilit√©s repr√©sentent une menace subtile mais critique car elles peuvent cr√©er des portes d√©rob√©es invisibles,
durablement int√©gr√©es dans le fonctionnement interne du mod√®le, √©chappant aux protections classiques bas√©es sur les 
prompts ou les sorties.

</details>
<br/>
<br/>
<details>
<summary>D√©sinformation (LLM09)</summary>
La d√©sinformation (Misinformation) d√©signe la capacit√© des mod√®les de langage (LLM) √† g√©n√©rer des contenus faux, 
inexactes ou trompeurs qui peuvent sembler cr√©dibles √† premi√®re vue. Cette vuln√©rabilit√© provient souvent d‚Äôerreurs, 
de biais ou de limites dans les donn√©es d‚Äôentra√Ænement, o√π le mod√®le "devine" ou hallucine des r√©ponses m√™me en 
l‚Äôabsence de faits confirm√©s.


Cette d√©sinformation peut mener √† des risques s√©rieux, tels que des atteintes √† la s√©curit√©, des dommages √† la 
r√©putation d‚Äôorganisations, ou des responsabilit√©s l√©gales si des d√©cisions sont prises sur la base d‚Äôinformations 
erron√©es. Par exemple, un LLM peut fournir un num√©ro d‚Äôurgence incorrect dans un contexte critique, ce qui pourrait 
mettre la vie des utilisateurs en danger.


Pour limiter ce risque, il est conseill√© d‚Äôutiliser des sources fiables et valid√©es pour l‚Äôentra√Ænement, de 
r√©guli√®rement v√©rifier l‚Äôexactitude des donn√©es, d‚Äôint√©grer des API de fact-checking en temps r√©el, et de filtrer ou 
valider les sorties du mod√®le avant leur publication ou utilisation. Un contr√¥le humain (human-in-the-loop) est aussi 
recommand√© pour approuver les r√©sultats dans des contextes √† risque √©lev√©.


La d√©sinformation est un enjeu cl√© pour la confiance dans les syst√®mes bas√©s sur les LLM, et n√©cessite des approches 
combinant techniques, processus et sensibilisation des utilisateurs.
</details>

<details>
<summary>Consommation illimit√©e (LLM10)</summary>
La consommation illimit√©e (Unbounded Consumption) fait r√©f√©rence √† une vuln√©rabilit√© o√π une application utilisant un 
grand mod√®le de langage (LLM) permet √† des utilisateurs de g√©n√©rer des requ√™tes ou des entr√©es excessives et non 
contr√¥l√©es. Cela entra√Æne une utilisation abusive des ressources computationnelles, comme la m√©moire et le CPU, pouvant
provoquer des d√©nis de service (DoS), des d√©gradations de service ou des co√ªts financiers tr√®s √©lev√©s.


Les attaquants exploitent cette faille en soumettant des entr√©es longues ou nombreuses, d√©clenchant des traitements 
lourds, souvent dans des environnements cloud, ce qui peut saturer les ressources et rendre le service indisponible pour
les utilisateurs l√©gitimes. Cette consommation excessive peut aussi conduire √† un vol indirect de propri√©t√© 
intellectuelle par extraction ou clonage du mod√®le.

Pour att√©nuer ce risque, il est recommand√© d‚Äôimposer des limites strictes sur la taille et le nombre des requ√™tes, de 
surveiller et enregistrer en continu l‚Äôusage des ressources, d‚Äôimpl√©menter des contr√¥les d‚Äôacc√®s rigoureux, ainsi que 
des strat√©gies de charge et d‚Äô√©quilibrage pour g√©rer les pics d‚Äôutilisation. Le syst√®me doit √™tre con√ßu pour d√©grader 
ses performances de mani√®re ma√Ætris√©e en cas de surcharge, plut√¥t que de tomber en panne compl√®te.

En somme, la consommation illimit√©e est un risque majeur car elle impacte directement la disponibilit√©, la s√©curit√© 
√©conomique et la r√©silience des applications bas√©es sur des LLMs.

</details>

## Soyez SAIF avec le Secure AI Framework


### Cadre Secure AI Framework (SAIF) de Google

Le **Secure AI Framework (SAIF)** de **Google** est un cadre conceptuel d√©velopp√© pour assurer la s√©curit√© tout au long 
du cycle de vie des syst√®mes d‚Äôintelligence artificielle, depuis la collecte des donn√©es jusqu‚Äôau d√©ploiement des mod√®les.
Il a √©t√© √©labor√© pour traiter les enjeux majeurs de gestion des risques, de s√©curit√© et de confidentialit√© propres aux 
mod√®les d‚ÄôIA et de machine learning. Le **SAIF** vise √† ce que ces syst√®mes soient ¬´ secure by default ¬ª d√®s 
leur mise en ≈ìuvre, en int√©grant des mesures de protection essentielles d√®s la phase de conception et tout au long du 
processus de d√©veloppement jusqu'au d√©ploiement en production.

**SAIF** s‚Äôinscrit dans la d√©marche g√©n√©rale de **Google** pour une IA responsable, align√©e sur des principes tels que: 
 - la s√©curit√©
 - l‚Äô√©quit√©
 - l‚Äôinterpr√©tabilit√©
 - la protection de la vie priv√©e. 

Le framework propose des normes et des 
contr√¥les concrets pour la construction, l‚Äô√©valuation et le d√©ploiement des syst√®mes d‚ÄôIA, en int√©grant la s√©curit√© de 
fa√ßon transverse.


### Les quatre grandes cat√©gories du SAIF

SAIF structure le d√©veloppement s√©curitaire de l‚ÄôIA autour de quatre axes majeurs, chacun √©tant associ√© √† plusieurs 
composants techniques cl√©s:

 - **Donn√©es** : concerne les sources de donn√©es, les processus de filtrage et de pr√©paration, ainsi que les ensembles 
utilis√©s pour l‚Äôentra√Ænement des mod√®les.
 - **Infrastructure** : englobe le mat√©riel, la s√©curit√© de l‚Äôh√©bergement, les frameworks et le code n√©cessaires √† 
l‚Äôentra√Ænement, le stockage des donn√©es et des mod√®les, ainsi que le d√©ploiement (Model Serving).
 - **Mod√®le** : inclut le mod√®le lui-m√™me (code et poids), la gestion des entr√©es (input handling) pour prot√©ger contre 
les entr√©es malicieuses, et la gestion des sorties (output handling) pour √©viter les expositions de donn√©es ou des 
comportements inattendus.
 - **Application** : se rapporte √† l‚Äôensemble applicatif qui interagit avec le mod√®le, y compris les agents ou plugins 
qui peuvent pr√©senter des risques additionnels.


### Les six √©l√©ments fondamentaux du SAIF

Le SAIF est articul√© autour de six √©l√©ments fondamentaux qui doivent √™tre continuellement mis en ≈ìuvre :

1. **√âtablir des bases de s√©curit√© solides pour l‚Äô√©cosyst√®me IA** : appliquer les principes traditionnels de s√©curit√© √† 
l‚Äôensemble de l‚Äô√©cosyst√®me IA, notamment pour l‚Äôinfrastructure, les codes sources et la cha√Æne d‚Äôapprovisionnement 
logicielle.
2. **√âtendre la d√©tection et la r√©ponse pour inclure l‚ÄôIA dans l‚Äôunivers des menaces de l‚Äôorganisation** : surveiller 
les entr√©es/sorties des syst√®mes IA et int√©grer l‚ÄôIA dans les dispositifs classiques de d√©tection et de r√©ponse aux 
incidents.
3. **Automatiser les d√©fenses pour suivre l‚Äô√©volution des menaces** : tirer parti des innovations IA pour contrer les 
attaques √† grande √©chelle et automatiser la protection.
4. **Harmoniser les contr√¥les au niveau des plateformes** : assurer une coh√©rence des politiques de s√©curit√© et des 
contr√¥les sur l‚Äôensemble des services et plateformes IA.
5. **Adapter les contr√¥les pour cr√©er des boucles de r√©troaction rapides** : ajuster et faire √©voluer rapidement les 
m√©canismes de s√©curit√© lors du d√©ploiement ou de l‚Äô√©volution des IA.
6. **Contextualiser les risques au sein des processus m√©tier** : int√©grer l‚Äôanalyse des risques IA dans la gestion 
globale des risques de l‚Äôorganisation


### Cartographie des risques et contr√¥les SAIF

SAIF donne acc√®s √† une cartographie d√©taill√©e des risques sp√©cifiques √† l‚ÄôIA, tels que :

- empoisonnement de donn√©es,
- utilisation de donn√©es non autoris√©es,
- alt√©ration du code ou des poids du mod√®le,
- exfiltration de mod√®les,
- attaque par d√©ni de service,
- divulgation ou inf√©rence de donn√©es sensibles, etc.

<a href="https://saif.google/secure-ai-framework/saif-map" target="_blank">
  <img src="img/saif-map.png" alt="SAIF">
</a>

<a href="https://saif.google/secure-ai-framework/saif-map" target="_blank"><em>source: saif.google</em></a>

√Ä chaque risque, des contr√¥les et mesures de mitigation sont propos√©s, ainsi que la d√©signation du responsable 
(cr√©ateur du mod√®le ou consommateur du mod√®le). Par exemple :

- Validation et assainissement des entr√©es/sorties
- Entra√Ænement et tests adversariaux
- Gestion des acc√®s, journalisation, surveillance continue

La cartographie SAIF permet d‚Äôidentifier o√π chaque risque √©merge (introduction), par o√π il peut √™tre exploit√© 
(exposition), et √† quel niveau il peut √™tre mitig√© (√©l√©ment ou contr√¥le associ√©).

### Mise en ≈ìuvre et communaut√© SAIF

**Google** met **SAIF** √† disposition comme point de r√©f√©rence pour les entreprises, gouvernements et organisations, avec un 
centre de ressources (saif.google) proposant des guides d‚Äôauto√©valuation, des contr√¥les de s√©curit√©, et un espace 
communautaire. Google travaille en coalition ([The Coalition for Secure AI](https://www.coalitionforsecureai.org/)) 
avec d‚Äôautres acteurs majeurs du secteur tels qu'**Amazon**, **Cisco**, **IBM**, **Intel**, **Microsoft**, **NVIDIA**, 
**OpenAI**,.. pour faire progresser l‚Äôadoption de ce framework, dans le but de s√©curiser l‚ÄôIA au b√©n√©fice de tous.

<details>
  <summary>The Coalition for Secure AI</summary>
<a href="https://www.coalitionforsecureai.org/" target="_blank">
  <img src="img/CoSAI.png" alt="image" width="450" style="transition:0.3s;">
</a>
</details>

## MITRE ATLAS, le fil d'Ariane des techniques d'attaque sur l'IA

<a href="https://www.riskinsight-wavestone.com/2024/11/lutilisation-pratique-du-cadre-atlas-de-mitre-pour-les-equipes-du-rssi/" target="_blank">
  <img src="https://www.riskinsight-wavestone.com/wp-content/uploads/2024/11/MITRE-Figure-1.png" alt="MITRE ATLAS" >
</a>

<a href="https://www.riskinsight-wavestone.com/2024/11/lutilisation-pratique-du-cadre-atlas-de-mitre-pour-les-equipes-du-rssi/" target="_blank"><em>source: riskinsight-wavestone.com</em></a>

### Objectif du MITRE ATLAS

Le **MITRE ATLAS** (Adversarial Tactics, Techniques, and Common Knowledge for AI Systems) est un cadre de r√©f√©rence 
international con√ßu pour identifier, classifier et att√©nuer les menaces adverses ciblant les syst√®mes d‚ÄôIA et 
d‚Äôapprentissage automatique. 

L‚Äôobjectif d‚ÄôATLAS est de fournir une base de connaissances structur√©e sur les tactiques 
et techniques employ√©es par les attaquants contre l‚ÄôIA, facilitant ainsi la protection proactive de ces syst√®mes et 
l‚Äôoptimisation de leur s√©curit√© dans l‚Äô√©cosyst√®me de l‚Äôentreprise.

### Cadre de r√©f√©rence

Le [**MITRE ATLAS**](https://atlas.mitre.org/) s‚Äôinspire fortement du c√©l√®bre cadre [**MITRE ATT&CK**](https://attack.mitre.org/), utilis√© dans la cybers√©curit√© traditionnelle pour 
cartographier les menaces et les actions adverses sur les syst√®mes d‚Äôinformation. ATLAS transpose cette d√©marche √† l‚ÄôIA, 
en se focalisant sur les risques, vuln√©rabilit√©s et techniques d‚Äôattaque sp√©cifiques aux technologies IA et machine 
learning.

### √âl√©ments fondamentaux du MITRE ATLAS

ATLAS est structur√© autour de plusieurs √©l√©ments cl√©s :

- **Tactiques** : Les objectifs de haut niveau poursuivis par les attaquants (ex. : √©vasion, compromission des donn√©es 
d‚Äôentra√Ænement, acc√®s initial).

- **Techniques** : Les m√©thodes concr√®tes permettant d‚Äôatteindre ces objectifs (ex. : data poisoning, model extraction, 
prompt injection).

- **Proc√©dures** : Exemples r√©els et cas d‚Äôutilisation illustrant comment ces techniques ont √©t√© appliqu√©es dans le 
monde r√©el.

- **√âtudes de cas** : Documentation d‚Äôattaques effectives sur des syst√®mes d‚ÄôIA pour enrichir continuellement la base 
de connaissances


### Comment l'utiliser
Le **MITRE ATLAS** est con√ßu pour √™tre utilis√© par les professionnels de la s√©curit√©, les chercheurs et les d√©veloppeurs. Il
faut voir **ATLAS** comme un tableau de bord structurant pour les √©quipes qui sont en charge de la s√©curit√© IA.

Les utilisateurs peuvent naviguer dans le r√©f√©rentiel pour identifier les menaces pertinentes √† leur contexte, comme:

- **Cartographie des menaces** : Les √©quipes de s√©curit√© mod√©lisent les risques et menaces qui p√®sent sur leur syst√®me d‚ÄôIA 
en utilisant la matrice ATLAS pour anticiper les techniques d‚Äôattaque possibles.
- **√âvaluation de la couverture d√©fensive** : √Ä l‚Äôaide d‚Äôoutils comme **ATLAS Navigator**, il est possible de visualiser 
quelles techniques d‚Äôattaque sont d√©j√† couvertes par les contr√¥les existants, lesquelles n√©cessitent des ajustements, 
ou pour lesquelles il faut cr√©er de nouveaux contr√¥les sp√©cifiques √† l‚ÄôIA.
- **D√©tection et r√©ponse aux incidents** : Lorsqu‚Äôun incident impliquant un syst√®me IA survient, ATLAS permet de tracer 
le d√©roul√© de l‚Äôattaque, d‚Äôidentifier les tactiques et techniques employ√©es, et de cibler la r√©ponse technique et 
organisationnelle.
- **Formation et sensibilisation** : **ATLAS** sert de support pour former les data scientists, ing√©nieurs IA et √©quipes 
SOC (Security Operations Center) aux menaces √©mergentes en IA, ouvrant la voie √† une collaboration interfonctionnelle √©troite entre m√©tiers et 
s√©curit√©.
- **D√©veloppement de cas d‚Äôusage d√©fensifs** : Les techniques document√©es dans ATLAS servent de base √† la cr√©ation de 
r√®gles de d√©tection, de sc√©narios de tests d‚Äôintrusion et de plans de mitigation sp√©cifiques aux syst√®mes d‚ÄôIA.


## R√©glementation l√©gislative des LLM

Entre 2013 et 2023, les entreprises am√©ricaines ont attir√© un volume de capitaux priv√©s plus de six fois sup√©rieur √† 
celui investi dans les entreprises europ√©ennes, favorisant ainsi l‚Äô√©mergence d‚Äôun √©cosyst√®me incomparable en mati√®re 
d‚Äôinnovation dans le domaine de l‚Äôintelligence artificielle.

√Ä titre de comparaison, les soci√©t√©s **am√©ricaines** ont lev√© environ **486,1 milliards de dollars** de financements priv√©s sur 
cette p√©riode, contre seulement **75,7 milliards** pour leurs homologues **europ√©ennes**.

<details>
  <summary>Comparatif graphique : 10 ans d‚Äôinvestissements en IA aux √âtats-Unis et dans l‚ÄôUnion europ√©enne.</summary>

<a href="https://actonline.org/2025/06/02/to-win-the-ai-race-congress-must-learn-from-europes-missteps/" target="_blank">
  <img src="https://actonline.org/wp-content/uploads/AI-blog-.png" alt="image" width="450" style="transition:0.3s;">
</a>
</details>

Ainsi, ces derni√®res ann√©es, de nombreux pays ont mis en place de nouvelles r√©gulations afin de faire face aux d√©rives 
li√©es aux technologies d‚Äôintelligence artificielle, notamment pour freiner la diffusion de la d√©sinformation et des 
discours de haine.
Voici un aper√ßu des approches adopt√©es par les √âtats-Unis et l‚ÄôUnion europ√©enne en mati√®re de r√©gulation des mod√®les de langage de grande taille (LLM).

### Enjeux et principes

- **√âquilibre responsabilit√©/innovation :** R√©guler les LLM exige de maintenir un juste √©quilibre entre la 
responsabilisation des acteurs (d√©veloppeurs, d√©ployeurs, utilisateurs) et la pr√©servation de l‚Äôinnovation. Les LLM 
pr√©sentent des avantages majeurs (√©ducation, accessibilit√©, cr√©ativit√©) mais comportent aussi des risques, comme la 
g√©n√©ration de contenus pr√©judiciables.


- **D√©finition de la responsabilit√© :** L‚Äôidentification des responsables des contenus g√©n√©r√©s par les LLM reste un 
enjeu. La responsabilit√© peut-elle incomber au d√©veloppeur, √† celui qui d√©ploie la solution, ou √† l‚Äôutilisateur ?


- **Respect des droits fondamentaux :** La lutte contre les abus ne doit pas compromettre des droits humains tels que la 
libert√© d‚Äôexpression. Les r√©glementations doivent donc viser la protection sans imposer une censure excessive.


## √âtats-Unis: une r√©gulation sectorielle et centr√©e sur la libert√© d‚Äôexpression

- **Libert√© d‚Äôexpression :** Aux √âtats-Unis, la diffusion d‚Äôinformations fausses ou controvers√©es, sauf en cas de 
diffamation, d‚Äôincitation √† la violence ou de fraude, reste prot√©g√©e par le premier amendement, rendant difficile 
l‚Äôinstauration de mesures de fond contre la d√©sinformation.


- **Take It Down Act :** Cette l√©gislation f√©d√©rale r√©cente cible la diffusion sur Internet de contenus abusifs, 
particuli√®rement les deepfakes et images intimes non consenties, y compris si elles sont g√©n√©r√©es par l‚ÄôIA. Elle impose 
aux plateformes un dispositif de signalement rapide et des obligations de retrait sous peine de sanctions administratives. 
La loi vient combler une lacune juridique majeure en mati√®re de deepfakes.


- **Obligations pour les plateformes :** Les r√©seaux sociaux et sites d‚Äôh√©bergement doivent retirer, dans un d√©lai court, 
les contenus signal√©s comme ill√©gaux. En cas de non-respect, ils s‚Äôexposent √† des sanctions importantes.  


- **Bonnes pratiques et autor√©gulation :** Les organismes publics comme le [NIST (National Institute of Standards and 
Technology)](https://www.nist.gov/) recommandent des pratiques de gestion des risques (AI RMF). D‚Äôautre part, 
la [Federal Trade Commission (FTC)](https://www.ftc.gov/) peut agir contre les pratiques commerciales trompeuses recourant √† l‚ÄôIA.


## Union europ√©enne: un encadrement structur√© et fond√© sur les risques
La r√©gulation europ√©enne s‚Äôarticule autour de deux textes principaux :

### Digital Services Act (DSA)

- **Obligations g√©n√©rales :** M√©canismes de signalement et suppression des contenus ill√©gaux ; droit de recours pour les 
utilisateurs en cas de suppression abusive.


- **Champ d‚Äôapplication large :** S‚Äôapplique √† tous les fournisseurs de services num√©riques op√©rant dans l‚ÄôUE, m√™me 
s‚Äôils sont bas√©s √† l‚Äô√©tranger.

- **√âvaluations de risques r√©currentes :** Pour certains acteurs majeurs, le DSA impose des √©valuations r√©guli√®res sur 
des probl√©matiques comme la d√©sinformation. Les plateformes doivent agir efficacement pour att√©nuer ces risques 
(modification d‚Äôalgorithmes, transparence accrue, etc.).


- **Renforcement de la transparence et des droits fondamentaux :** Protection contre les contenus nuisibles, respect de 
la vie priv√©e et de la libert√© d‚Äôexpression, publication claire des politiques de mod√©ration et de publicit√©.

### AI Act

- Une approche fond√©e sur 4 niveaux de risques :

  - **Risque inacceptable :** Syst√®mes interdits (ex. : social scoring, manipulation de masse).
  - **Risque √©lev√© :** Secteurs critiques (sant√©, √©ducation, s√©curit√©). Obligations strictes : gestion des risques, 
  gouvernance des donn√©es, supervision humaine.
  - **Risque limit√© :** Syst√®mes interagissant avec le public ou g√©n√©rant du contenu, comme les LLM. Exigences de 
  transparence (divulguer si un contenu est g√©n√©r√© par IA), documentation, mesures pour pr√©venir les usages abusifs.
  - **Risque minimal :** Applications simples telles que filtres anti-spam, largement non r√©glement√©es.


- **Innovations sp√©cifiques :** L‚ÄôAI Act pr√©voit des obligations particuli√®res pour les mod√®les d‚ÄôIA √† usage g√©n√©ral 
pr√©sentant un risque syst√©mique important (capacit√©s computationnelles massives).


- **P√©nalit√©s et contr√¥les :** Les contrevenants s‚Äôexposent √† des contr√¥les accrus et √† des sanctions financi√®res 
dissuasives.


### Points de convergence et divergences

Voici un tableau r√©capitulatif des diff√©rences et similitudes entre les approches r√©glementaires des √âtats-Unis et 
de l‚ÄôUnion europ√©enne concernant les LLM :

| Points                            | √âtats-Unis                                                                                            | Union europ√©enne                                                                                             |
|-----------------------------------|-------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|
| **Priorit√© fondamentale**         | Libert√© d‚Äôexpression pr√©serv√©e, interventions cibl√©es sur des enjeux pr√©cis (deepfakes, fraude).      | Approche globale couvrant l‚Äôensemble du cycle de vie de l‚ÄôIA selon le niveau de risque .                     |
| **Philosophie de r√©gulation**     | Focalis√©e sur les cas particuliers et d√©fense du Premier Amendement.                                  | Pr√©ventive, syst√©mique, avec exigence de transparence, documentation et gestion du risque √† chaque √©tape .   |
| **Responsabilisation**            | Obligations cibl√©es sur certaines plateformes ; sanctions et normes variables selon les √âtats et cas. | Responsabilisation claire et harmonis√©e pour plateformes et fournisseurs, r√©gime de sanction structur√© .     |
| **Transparence et documentation** | Exigences limit√©es √† certains cas sp√©cifiques (deepfakes, fraude), pas de cadre g√©n√©ral harmonis√©.    | Forte exigence de transparence, documentation, acc√®s public √† l‚Äôinformation pour syst√®mes IA √† risque .      |
| **Harmonisation des sanctions**   | Fragment√©e, grandes variations selon √âtats f√©d√©r√©s et interventions f√©d√©rales ponctuelles.            | Sanctions harmonis√©es, r√©guli√®rement renforc√©es et uniformis√©es au niveau de l‚ÄôUE.                           |

Cette opposition illustre la diversit√© des strat√©gies de r√©gulation, oscillant entre respect des libert√©s et pr√©vention 
efficace des abus technologiques.

## √âtape suivante

- [√âtape 5](step_5.md)


## Ressources

| Information                                                                                          | Lien                                                                                                                                                                                                                                                                                                                                                         |
|------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| OWASP Top 10 for LLM Applications  2025                                                              | [https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-v2025.pdf](https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-v2025.pdf)                                                                                                               |
| Explained: The OWASP Top 10 for Large Language Model Applications                                    | [https://www.youtube.com/watch?v=cYuesqIKf9A](https://www.youtube.com/watch?v=cYuesqIKf9A)                                                                                                                                                                                                                                                                   |
| Model theft: Meta LLaMA leak                                                                         | [https://learn.snyk.io/lesson/model-theft-llm/](https://learn.snyk.io/lesson/model-theft-llm/)                                                                                                                                                                                                                                                               |
| Secure AI Framework (SAIF)                                                                           | [https://saif.google/](https://saif.google/)                                                                                                                                                                                                                                                                                                                 |
| Google lance un framework pour s√©curiser les mod√®les d'IA                                            | [https://www.lemondeinformatique.fr/actualites/lire-google-lance-un-framework-pour-securiser-les-modeles-d-ia-90694.html](https://www.lemondeinformatique.fr/actualites/lire-google-lance-un-framework-pour-securiser-les-modeles-d-ia-90694.html)                                                                                                           |
| Pr√©sentation de la s√©curit√© dans le monde de l'IA                                                    | [https://www.cloudskillsboost.google/paths/1283/course_templates/1147?locale=fr](https://www.cloudskillsboost.google/paths/1283/course_templates/1147?locale=fr)                                                                                                                                                                                             |
| What Is Google's Secure AI Framework (SAIF)?                                                         | [https://www.paloaltonetworks.com/cyberpedia/google-secure-ai-framework](https://www.paloaltonetworks.com/cyberpedia/google-secure-ai-framework)                                                                                                                                                                                                             |
| Secure AI Framework Approach                                                                         | [https://kstatic.googleusercontent.com/files/00e270b1cccb1f37302462a162c171d86f293a84de54036e0021e2fe0253cf05623bae2a62751b0840667bc6c8412fd70f45c9485972dc370be8394fae922d31](https://kstatic.googleusercontent.com/files/00e270b1cccb1f37302462a162c171d86f293a84de54036e0021e2fe0253cf05623bae2a62751b0840667bc6c8412fd70f45c9485972dc370be8394fae922d31) |
| Securing the AI Pipeline                                                                             | [https://cloud.google.com/blog/topics/threat-intelligence/securing-ai-pipeline/?hl=en](https://cloud.google.com/blog/topics/threat-intelligence/securing-ai-pipeline/?hl=en)                                                                                                                                                                                 |
| Introducing the Coalition for Secure AI (CoSAI) and founding member organizations                    | [https://blog.google/technology/safety-security/google-coalition-for-secure-ai/](https://blog.google/technology/safety-security/google-coalition-for-secure-ai/)                                                                                                                                                                                             |
| MITRE ATLAS                                                                                          | [https://atlas.mitre.org/](https://atlas.mitre.org/)                                                                                                                                                                                                                                                                                                         |
| Anatomy of an AI ATTACK: MITRE ATLAS                                                                 | [https://www.youtube.com/watch?v=QhoG74PDFyc](https://www.youtube.com/watch?v=QhoG74PDFyc)                                                                                                                                                                                                                                                                   |
| MITRE ATLAS‚Ñ¢ Introduction                                                                            | [https://www.youtube.com/watch?v=3FN9v-y-C-w](https://www.youtube.com/watch?v=3FN9v-y-C-w)                                                                                                                                                                                                                                                                   |
| L‚Äôutilisation pratique du cadre ATLAS de MITRE pour les √©quipes du RSSI                              | [https://www.riskinsight-wavestone.com/2024/11/lutilisation-pratique-du-cadre-atlas-de-mitre-pour-les-equipes-du-rssi/](https://www.riskinsight-wavestone.com/2024/11/lutilisation-pratique-du-cadre-atlas-de-mitre-pour-les-equipes-du-rssi/)                                                                                                               |
| Introduction √† l'apprentissage automatique contradictoire et au cadre MITRE ATLAS                    | [https://www.infosecured.ai/fr/i/mlsec/adversarial-machine-learning-mitre-atlas-framework/](https://www.infosecured.ai/fr/i/mlsec/adversarial-machine-learning-mitre-atlas-framework/)                                                                                                                                                                       |
| MITRE ATLAS Framework 2025 ‚Äì Guide to Securing AI Systems                                            | [https://www.practical-devsecops.com/mitre-atlas-framework-guide-securing-ai-systems/](https://www.practical-devsecops.com/mitre-atlas-framework-guide-securing-ai-systems/)                                                                                                                                                                                 |
| ATLAS Navigator                                                                                      | [https://github.com/mitre-atlas/atlas-navigator](https://github.com/mitre-atlas/atlas-navigator)                                                                                                                                                                                                                                                             |
| Faut-il investir sur la tech europ√©enne ? L'analyse d'un insider - Finary Talk #60 & Olivier Coste - | [https://youtu.be/Tw-HRXlVIa0?si=ZRHWRjy_vzHcQ6Af](https://youtu.be/Tw-HRXlVIa0?si=ZRHWRjy_vzHcQ6Af)                                                                                                                                                                                                                                                         |
| AI Act                                                                                               | [https://artificialintelligenceact.eu/fr/](https://artificialintelligenceact.eu/fr/)                                                                                                                                                                                                                                                                         |
| AI Act - European Commission                                                                         | [https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai)                                                                                                                                                                                                       |
| Digital Services Act (DSA)                                                                           | [https://www.vie-publique.fr/eclairage/285115-dsa-le-reglement-sur-les-services-numeriques-ou-digital-services-act](https://www.vie-publique.fr/eclairage/285115-dsa-le-reglement-sur-les-services-numeriques-ou-digital-services-act)                                                                                                                       |
| Take It Down Act                                                                                     | [https://www.congress.gov/bill/119th-congress/senate-bill/146](https://www.congress.gov/bill/119th-congress/senate-bill/146)                                                                                                                                                                                                                                 |
| NIST AI RMF                                                                                          | [https://www.nist.gov/itl/ai-risk-management-framework](https://www.nist.gov/itl/ai-risk-management-framework)                                                                                                                                                                                                                                               |
| Artificial Intelligence Regulation Threatens Free Expression                                         | [https://www.cato.org/briefing-paper/artificial-intelligence-regulation-threatens-free-expression](https://www.cato.org/briefing-paper/artificial-intelligence-regulation-threatens-free-expression)                                                                                                                                                         |
| AI regulation: EU vs. USA - opportunities and challenges for companies                               | [https://www.tucan.ai/blog/ai-regulation-eu-vs-usa-opportunities-and-challenges-for-companies/](https://www.tucan.ai/blog/ai-regulation-eu-vs-usa-opportunities-and-challenges-for-companies/)                                                                                                                                                               |
| Comparing the EU AI Act to Proposed AI-Related Legislation in the US                                 | [https://businesslawreview.uchicago.edu/online-archive/comparing-eu-ai-act-proposed-ai-related-legislation-us](https://businesslawreview.uchicago.edu/online-archive/comparing-eu-ai-act-proposed-ai-related-legislation-us)                                                                                                                                 |
| Take It Down Act : la loi contre le revenge porn et les deepfakes pornographiques.                   | [https://toutsurlacyber.fr/take-it-down-act-revenge-porn-et-deepfakes/](https://toutsurlacyber.fr/take-it-down-act-revenge-porn-et-deepfakes/)                                                                                                                                                                                                               |
| Le minist√®re am√©ricain du Commerce publie un outil de mesure des risques inh√©rents aux LLM           | [https://www.usine-digitale.fr/article/le-ministere-americain-du-commerce-publie-un-outil-de-mesure-des-risques-inherents-aux-llm.N2216752](https://www.usine-digitale.fr/article/le-ministere-americain-du-commerce-publie-un-outil-de-mesure-des-risques-inherents-aux-llm.N2216752)                                                                       |
| Entr√©e en vigueur du r√®glement europ√©en sur l‚ÄôIA : les premi√®res questions-r√©ponses de la CNIL       | [https://www.cnil.fr/fr/entree-en-vigueur-du-reglement-europeen-sur-lia-les-premieres-questions-reponses-de-la-cnil](https://www.cnil.fr/fr/entree-en-vigueur-du-reglement-europeen-sur-lia-les-premieres-questions-reponses-de-la-cnil)                                                                                                                     |
| GenAI Incident Response Guide                                                                        | [https://genai.owasp.org/resource/genai-incident-response-guide-1-0/](https://genai.owasp.org/resource/genai-incident-response-guide-1-0/)                                                                                                                                                                                                                   |
