# Cadres de S√©curit√© R√©f√©rents

[<img src="img/step4.jpg" alt="Arwen" >](https://www.youtube.com/watch?v=fd2AO0gr3Rc)
> "if you want him come and claim him", Arwen, LOTR - The Followship of the Ring

## üéØ Objectifs de cette √©tape
- Comprendre les cadres de s√©curit√© existants pour les LLM (mod√®les de langage de grande taille)
- Identifier les principaux risques et vuln√©rabilit√©s li√©s √† l‚Äôutilisation des LLM
- D√©couvrir les frameworks et r√©f√©rentiels de s√©curit√© d√©di√©s (OWASP Top 10, SAIF, MITRE ATLAS)
- Appr√©hender les r√©glementations l√©gislatives encadrant les LLM (√âtats-Unis, Union europ√©enne)
- Acc√©der √† des ressources pour approfondir la s√©curisation des applications IA


## Sommaire

- [OWASP Top 10 for LLM Applications](#owasp-top-10-for-llm-applications)
  - [Plus en d√©tail](#plus-en-d√©tail)


- [Soyez SAIF avec le Secure AI Framework](#soyez-saif-avec-le-secure-ai-framework)
  - [Cadre Secure AI Framework (SAIF) de Google](#cadre-secure-ai-framework-saif-de-google)
  - [Les quatre grandes cat√©gories du SAIF](#les-quatre-grandes-cat√©gories-du-saif)
  - [Les six √©l√©ments fondamentaux du SAIF](#les-six-√©l√©ments-fondamentaux-du-saif)
  - [Cartographie des risques et contr√¥les SAIF](#cartographie-des-risques-et-contr√¥les-saif)
  - [Mise en ≈ìuvre et communaut√© SAIF](#mise-en-≈ìuvre-et-communaut√©-saif)


- [MITRE ATLAS, le fil d'Ariane des techniques d'attaque sur l'IA](#mitre-atlas-le-fil-dariane-des-techniques-dattaque-sur-lia)
  - [Objectif du MITRE ATLAS](#objectif-du-mitre-atlas)
  - [Cadre de r√©f√©rence](#cadre-de-r√©f√©rence)
  - [√âl√©ments fondamentaux du MITRE ATLAS](#√©l√©ments-fondamentaux-du-mitre-atlas)
  - [Comment l'utiliser](#comment-lutiliser)


- [R√©glementation l√©gislative des LLM](#r√©glementation-l√©gislative-des-llm)
    - [Enjeux et principes](#enjeux-et-principes)
      - [√âtats-Unis: une r√©gulation sectorielle et centr√©e sur la libert√© d‚Äôexpression](#√©tats-unis-une-r√©gulation-sectorielle-et-centr√©e-sur-la-libert√©-dexpression)
      - [Union europ√©enne: un encadrement structur√© et fond√© sur les risques](#union-europ√©enne-un-encadrement-structur√©-et-fond√©-sur-les-risques)
        - [Digital Services Act (DSA)](#digital-services-act-dsa)
        - [AI Act](#ai-act)
    - [Points de convergence et divergences](#points-de-convergence-et-divergences)
  

- [Ressources](#ressources)

## OWASP Top 10 for LLM Applications

<a href="https://genai.owasp.org/2023/10/18/llm-to-10-v1-1/" target="_blank">
  <img src="https://genai.owasp.org/wp-content/uploads/2024/05/1697599021768.jpeg" alt="image" width="950" style="transition:0.3s;">
</a>
<a href="https://genai.owasp.org/2023/10/18/llm-to-10-v1-1/" target="_blank"><em>source: genai.owasp.org</em></a>


L‚Äô**OWASP Top 10 for Large Language Model Applications** est aujourd‚Äôhui l‚Äôoutil de r√©f√©rence pour recenser, analyser et 
att√©nuer les principaux risques de s√©curit√© propres √† l‚Äôutilisation des grands mod√®les de langage.


Cette liste √©labor√©e collectivement constitue un guide incontournable pour les d√©veloppeurs, architectes et responsables 
de la s√©curit√© d√©sireux d‚Äôint√©grer l‚ÄôIA g√©n√©rative de mani√®re fiable et s√©curis√©e dans leurs syst√®mes d‚Äôinformation. 
Ce classement a vu le jour gr√¢ce √† l‚Äôengagement de [John Sotiropoulos](https://www.linkedin.com/in/jsotiropoulos/), 
co-pilote du projet, et d‚Äô[Ads Dawson](https://www.linkedin.com/in/adamdawson0/), responsable technique en charge de la 
coordination de la r√©daction des aspects techniques du r√©f√©rentiel.

Voici une synth√®se des vuln√©rabilit√©s qui concernent sp√©cifiquement les LLM:

| IDENTIFIANT  | Description                                                                                                                                                                                                                    |
|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ‚ú® **LLM01**  | **Injection de prompt** : Les attaquants manipulent l'entr√©e du LLM directement ou indirectement pour provoquer un comportement malveillant ou ill√©gal.                                                                        |
| ‚ú® **LLM02**  | **Gestion non s√©curis√©e de la sortie** : La sortie du LLM est g√©r√©e de mani√®re non s√©curis√©e, entra√Ænant des vuln√©rabilit√©s d'injection telles que le Cross-Site Scripting (XSS), l'injection SQL ou l'injection de commandes. |
| **LLM03**    | **Empoisonnement des donn√©es d'entra√Ænement** : Les attaquants injectent des donn√©es malveillantes ou trompeuses dans les donn√©es d'entra√Ænement du LLM, compromettant ses performances ou cr√©ant des portes d√©rob√©es.         |
| ‚ú® **LLM04**  | **D√©ni de service du mod√®le** : Les attaquants fournissent au LLM des entr√©es provoquant une consommation excessive de ressources, causant potentiellement des perturbations du service.                                       |
| **LLM05**    | **Vuln√©rabilit√©s de la cha√Æne d'approvisionnement** : Les attaquants exploitent les vuln√©rabilit√©s dans n‚Äôimporte quelle partie de la cha√Æne d‚Äôapprovisionnement du LLM.                                                       |
| ‚ú® **LLM06**  | **Divulgation d‚Äôinformations sensibles** : Les attaquants trompent le LLM pour qu'il r√©v√®le des informations sensibles dans sa r√©ponse.                                                                                        |
| **LLM07**    | **Conception de plugins non s√©curis√©e** : Les attaquants exploitent des vuln√©rabilit√©s dans la s√©curit√© des plugins LLM.                                                                                                       |
| üìñ **LLM08** | **Acc√®s excessif (agency)** : Les attaquants exploitent l‚Äôacc√®s insuffisamment restreint du LLM √† des syst√®mes ou √† des actions sensibles.                                                                                     |
| üìñ **LLM09** | **D√©pendance excessive** : Une organisation d√©pend de mani√®re excessive des r√©sultats d‚Äôun LLM pour prendre des d√©cisions critiques, exposant ainsi la s√©curit√© √† des comportements inattendus du mod√®le.                      |
| üìñ **LLM10** | **Vol de mod√®le** : Les attaquants obtiennent un acc√®s non autoris√© au LLM, volant de la propri√©t√© intellectuelle et causant potentiellement des pertes financi√®res.                                                           |


### Plus en d√©tail

<details>
  <summary>Injection de prompt (LLM01)</summary>

L‚Äôinjection de prompt est une vuln√©rabilit√© de s√©curit√© qui survient lorsque qu‚Äôun utilisateur malintentionn√© parvient √† 
manipuler les instructions fournies en entr√©e √† un mod√®le de langage (LLM), l‚Äôamenant ainsi √† adopter un comportement 
impr√©vu ou non d√©sir√©.


Si certaines manipulations peuvent para√Ætre anodines ‚Äî par exemple, d√©tourner un chatbot d‚Äôassistance technique pour lui 
faire donner des recettes de cuisine ‚Äî d‚Äôautres peuvent avoir des cons√©quences bien plus graves. Cette technique peut 
√™tre exploit√©e pour inciter le LLM √† produire de fausses informations, des discours haineux ou encore du contenu 
nuisible, voire ill√©gal.


Dans certains cas, l‚Äôinjection de prompt peut √©galement permettre √† un attaquant d‚Äôextraire des donn√©es sensibles 
pr√©c√©demment communiqu√©es au mod√®le, compromettant ainsi la confidentialit√© des informations trait√©es.
</details>


<details>
  <summary>Gestion non s√©curis√©e de la sortie (LLM02)</summary>

Le texte g√©n√©r√© par un LLM doit √™tre consid√©r√© avec le m√™me niveau de m√©fiance que des donn√©es saisies par un utilisateur
non fiable. Si une application web ne valide ni ne nettoie correctement ces sorties, elle peut √™tre expos√©e √† des 
vuln√©rabilit√©s classiques telles que le Cross-Site Scripting (XSS), l‚Äôinjection SQL ou l‚Äôinjection de code.


Il est donc essentiel de s'assurer que le contenu g√©n√©r√© par le LLM respecte bien la syntaxe attendue et les valeurs 
autoris√©es. Par exemple, imaginons qu‚Äôun LLM soit utilis√© pour g√©n√©rer des requ√™tes √† partir d‚Äôinstructions fournies 
par l‚Äôutilisateur. Si un utilisateur √©crit ¬´ Donne-moi le contenu de l‚Äôarticle num√©ro 42 ¬ª, le mod√®le pourrait produire 
la requ√™te SQL suivante :

> ``` 
> SELECT * FROM articles WHERE id = 42;
> ```

Cette requ√™te peut ensuite √™tre envoy√©e √† la base de donn√©es pour afficher le contenu correspondant.


Cependant, ce type de fonctionnement introduit des risques importants. Au-del√† de l‚Äô√©ventualit√© d‚Äôattaques par injection SQL, il est imp√©ratif de soumettre chaque requ√™te g√©n√©r√©e √† des v√©rifications de plausibilit√©. Dans le cas contraire, un comportement inattendu pourrait survenir. Par exemple, si un attaquant parvient √† inciter le LLM √† produire une requ√™te telle que :
> ```
>  DROP TABLE articles;
> ```
 et l√†, toutes les donn√©es pourraient √™tre irr√©m√©diablement perdues.

En r√©sum√©, toute int√©gration d‚Äôun LLM dans une application, en particulier lorsqu‚Äôil g√©n√®re des instructions interpr√©tables par un syst√®me, doit s‚Äôaccompagner d‚Äôun strict contr√¥le de validation, au m√™me titre que pour n‚Äôimporte quelle entr√©e utilisateur.
</details>


<details>
  <summary>Empoisonnement des donn√©es d'entra√Ænement (LLM03)</summary>

La qualit√© et les performances d‚Äôun mod√®le de langage (LLM) d√©pendent en grande partie des donn√©es utilis√©es durant sa 
phase d'entra√Ænement. L‚Äôempoisonnement des donn√©es d‚Äôentra√Ænement (Training Data Poisoning) consiste √† manipuler tout 
ou partie de ces donn√©es afin d‚Äôintroduire des biais volontaires, incitant le mod√®le √† produire des r√©sultats incorrects
ou malveillants.


Selon l‚Äôusage du LLM ainsi compromis, les cons√©quences peuvent aller d‚Äôune perte de cr√©dibilit√© √† des vuln√©rabilit√©s 
critiques en mati√®re de s√©curit√©, notamment si le mod√®le g√©n√®re du code r√©utilis√© dans d‚Äôautres composants logiciels.


Pour r√©ussir une attaque par empoisonnement des donn√©es d‚Äôapprentissage, un attaquant doit d‚Äôabord avoir acc√®s au corpus
de donn√©es utilis√© pour entra√Æner le mod√®le. Lorsque l‚Äôentra√Ænement repose sur des donn√©es accessibles publiquement 
(comme du contenu web), il est crucial de les nettoyer et de v√©rifier leur int√©grit√© afin d‚Äô√©carter toute source de 
biais ou de contenu manipul√©.

Parmi les strat√©gies de mitigation √† adopter, on peut citer :
- La v√©rification fine et r√©guli√®re de la cha√Æne d‚Äôapprovisionnement des donn√©es d‚Äôentra√Ænement
- L‚Äô√©valuation de la l√©gitimit√© et de la provenance des sources
- L‚Äôimpl√©mentation de filtres capables d‚Äôidentifier et d‚Äôexclure les donn√©es incorrectes ou malveillantes

En somme, une attention rigoureuse port√©e √† la qualit√© des donn√©es d‚Äôentra√Ænement est essentielle pour garantir un 
comportement fiable et √©thique des LLM.
</details>


<details>
  <summary>D√©ni de service du mod√®le (LLM04)</summary>

Une attaque par d√©ni de service (DoS) visant un mod√®le de langage (LLM) fonctionne selon le m√™me principe que toute autre
attaque de ce type : elle vise √† perturber ou bloquer l'acc√®s au service pour les autres utilisateurs en r√©duisant sa disponibilit√©.


Comme les LLM n√©cessitent g√©n√©ralement une puissance de calcul √©lev√©e, une requ√™te volontairement con√ßue pour solliciter
√©norm√©ment de ressources peut facilement saturer le syst√®me. Si l‚Äôinfrastructure ne dispose pas de protections 
suffisantes ou de capacit√©s ad√©quates, ce type de surcharge peut provoquer une interruption compl√®te du service.


Pour se pr√©munir contre de telles attaques, la validation rigoureuse des entr√©es utilisateur est indispensable. 
Toutefois, en raison du caract√®re non d√©terministe et parfois impr√©visible des LLM, il n‚Äôest pas efficace de bloquer 
simplement certaines requ√™tes pr√©alablement identifi√©es comme malveillantes.

Une protection efficace repose donc sur une combinaison de mesures, notamment :
- La mise en place de limites strictes de fr√©quence d‚Äôacc√®s (rate limiting)
- Le suivi en temps r√©el de la consommation des ressources
- La d√©tection pr√©coce des comportements anormaux pouvant signaler un d√©but d‚Äôattaque

Ces pr√©cautions permettent d'assurer que le service reste disponible pour tous les utilisateurs, m√™me en cas de 
tentatives de saturation malveillantes.
</details>

<details>
  <summary>Vuln√©rabilit√©s de la cha√Æne d'approvisionnement (LLM05)</summary>

Les vuln√©rabilit√©s de la cha√Æne d‚Äôapprovisionnement dans le contexte des LLM concernent tous les √©l√©ments impliqu√©s dans
leur d√©veloppement ou leur d√©ploiement. Cela inclut notamment les jeux de donn√©es utilis√©s pour l‚Äôentra√Ænement (voir LLM03), 
les mod√®les pr√©entra√Æn√©s fournis par des tiers, ainsi que les plugins, extensions ou autres syst√®mes interagissant 
avec le LLM (cf. LLM07).


L‚Äôimpact de ces vuln√©rabilit√©s peut varier consid√©rablement, allant de simples dysfonctionnements √† des cons√©quences 
critiques. L‚Äôun des sc√©narios les plus courants est la fuite de donn√©es sensibles ou la divulgation de propri√©t√© 
intellectuelle, compromettant la confidentialit√© ou les actifs strat√©giques de l‚Äôorganisation.
</details>

<details>
  <summary>Divulgation d‚Äôinformations sensibles (LLM06)</summary>

Les mod√®les de langage (LLM) peuvent, de mani√®re involontaire, divulguer des donn√©es confidentielles dans leurs r√©ponses.
Une telle exposition peut entra√Æner un acc√®s non autoris√© √† des informations sensibles, des atteintes √† la vie priv√©e, 
voire des failles de s√©curit√©. Il est donc essentiel de restreindre strictement l'acc√®s aux informations que le LLM est 
autoris√© √† consulter.


Cela est particuli√®rement important lorsque le mod√®le est utilis√© pour traiter des donn√©es sensibles ou strat√©giques, 
comme des informations clients. Dans ces cas, les requ√™tes adress√©es au LLM doivent faire l‚Äôobjet de contr√¥les d‚Äôacc√®s 
rigoureux afin de limiter le risque de fuite de donn√©es.

Si le LLM a √©t√© entra√Æn√© ou affin√© √† l‚Äôaide d‚Äôun jeu de donn√©es personnalis√©, il est crucial de garder √† l‚Äôesprit qu‚Äôil 
peut √™tre manipul√© (par des attaques d'injection de prompt, par exemple) pour r√©v√©ler des √©l√©ments de ces donn√©es 
d‚Äôapprentissage. Ainsi, toute information sensible int√©gr√©e au corpus d‚Äôentra√Ænement doit √™tre soigneusement identifi√©e,
√©valu√©e en fonction de sa criticit√©, et prot√©g√©e en cons√©quence.

De plus, les donn√©es sensibles fournies au LLM via des "prompts" utilisateurs peuvent √™tre expos√©es par des attaques 
d‚Äôinjection (voir LLM01), m√™me si l‚Äôon a explicitement demand√© au mod√®le de garder ces informations confidentielles. 
Cela souligne la n√©cessit√© de mettre en place des mesures de s√©curit√© adapt√©es √† chaque point de contact entre 
l‚Äôutilisateur et le mod√®le.
</details>


<details>
  <summary>Conception de plugins non s√©curis√©e (LLM07)</summary>

Les mod√®les de langage (LLM) peuvent √™tre connect√©s √† d'autres syst√®mes ou services via des plugins. Cependant, si un 
plugin traite les r√©ponses g√©n√©r√©es par le LLM sans proc√©der √† une validation ou un filtrage rigoureux, cela peut 
entra√Æner des failles de s√©curit√© significatives.


Selon les fonctionnalit√©s offertes par le plugin, plusieurs types de vuln√©rabilit√©s connues sur le Web peuvent se 
produire, notamment :
- **Cross-Site Scripting (XSS)** : Si le plugin affiche du contenu g√©n√©r√© par le LLM sans validation, un attaquant peut 
injecter du code malveillant qui sera ex√©cut√© dans le navigateur de l‚Äôutilisateur.
- **Injection SQL** : Si le plugin utilise les r√©ponses du LLM pour interagir avec une base de donn√©es sans filtrage
- **fraude par requ√™tes c√¥t√© serveur (SSRF)** : Si le plugin permet au LLM d‚Äôacc√©der √† des ressources internes ou externes
sans restrictions, un attaquant peut exploiter cette fonctionnalit√© pour acc√©der √† des donn√©es sensibles ou ex√©cuter des 
actions non autoris√©es.
- **Ex√©cution de commandes √† distance (RCE)** : Si le plugin ex√©cute des commandes syst√®me bas√©es sur les r√©ponses du LLM

Pour √©viter ces risques, il est essentiel que tout √©change entre un LLM et son environnement d'ex√©cution ‚Äî notamment via
des plugins ‚Äî soit strictement contr√¥l√©, avec un m√©canisme de validation des sorties du mod√®le avant toute action ou 
interaction avec des composants externes.
</details>


<details>
  <summary>Acc√®s excessif (LLM08)</summary>

Des vuln√©rabilit√©s de s√©curit√© peuvent survenir lorsqu‚Äôun LLM dispose de plus d‚Äôautonomie ou de privil√®ges que 
n√©cessaire pour remplir sa fonction. √Ä l‚Äôinstar du **principe du moindre privil√®ge**, il est essentiel de limiter 
strictement les capacit√©s accord√©es au LLM afin de r√©duire sa surface d‚Äôexposition face aux attaques potentielles.


Par exemple, si un LLM est autoris√© √† interagir avec d‚Äôautres syst√®mes ou services, il convient de mettre en place une 
liste blanche (whitelisting) d√©finissant pr√©cis√©ment les ressources auxquelles il peut acc√©der. Il est 
√©galement crucial de bien d√©limiter les actions que le mod√®le est cens√© accomplir, en restreignant ses permissions √† 
cette finalit√© unique.

Prenons le cas d‚Äôun LLM connect√© √† une base de donn√©es SQL afin de r√©cup√©rer des informations √† la demande de 
l‚Äôutilisateur. Si son acc√®s √† la base n‚Äôest pas restreint, un attaquant pourrait le manipuler pour qu‚Äôil ex√©cute des 
instructions telles que DELETE ou INSERT, compromettant ainsi l'int√©grit√© des donn√©es.


En r√©sum√©, il est imp√©ratif de contr√¥ler avec pr√©cision les autorisations du LLM, tant au niveau des services 
accessibles que des actions autoris√©es, afin de garantir un usage s√©curis√© et ma√Ætris√© du mod√®le.
</details>


<details>
  <summary>D√©pendance excessive (LLM09)</summary>

En raison de leur mode de fonctionnement, les mod√®les de langage (LLM) sont naturellement sujets √† produire des 
informations erron√©es. Cela peut se traduire aussi bien par des affirmations factuellement inexactes que par du code 
contenant des erreurs ou des bugs.


Si un LLM est int√©gr√© dans les processus m√©tiers d‚Äôune organisation sans m√©canismes de v√©rification ad√©quats, les 
donn√©es incorrectes qu‚Äôil g√©n√®re peuvent devenir une source potentielle de vuln√©rabilit√©s, 
notamment dans des contextes sensibles ou critiques.


Il est donc essentiel de valider manuellement les informations fournies par le LLM avant toute utilisation 
op√©rationnelle. Cette √©tape de contr√¥le joue un r√¥le crucial pour √©viter la propagation d‚Äôerreurs, limiter les 
risques de faille de s√©curit√© et garantir la fiabilit√© du syst√®me dans son ensemble.
</details>


<details>
  <summary>Vol de mod√®le (LLM10)</summary>

Le vol de mod√®le se produit lorsqu‚Äôun attaquant parvient √† s‚Äôapproprier un LLM dans son int√©gralit√©, c‚Äôest-√†-dire √† 
extraire ses poids et ses param√®tres internes. Une fois en possession de ces √©l√©ments, l‚Äôattaquant est capable de 
reproduire fid√®lement le mod√®le d‚Äôorigine. Cela peut non seulement nuire √† la r√©putation de l‚Äôentit√© victime, mais 
aussi permettre √† l‚Äôattaquant de proposer un service √©quivalent √† moindre co√ªt, sans avoir eu √† supporter les lourds 
investissements en temps, en donn√©es et en ressources n√©cessaires √† l'entra√Ænement du LLM.


Pour se pr√©munir contre ce type de menace, il est essentiel de mettre en place des m√©canismes robustes 
d‚Äôauthentification et de contr√¥le d‚Äôacc√®s, afin d‚Äôemp√™cher toute extraction ou utilisation non autoris√©e du mod√®le.

Par exemple, la fuite du mod√®le Meta LLaMA (2023) : Les poids du mod√®le LLaMA de Meta, initialement sous acc√®s restreint,
ont √©t√© partag√©s publiquement √† la suite d‚Äôun acte interne, illustrant un cas r√©el de vol de mod√®le avec diffusion 
massive et non autoris√©e sur internet.

</details>



## Soyez SAIF avec le Secure AI Framework


### Cadre Secure AI Framework (SAIF) de Google

Le **Secure AI Framework (SAIF)** de **Google** est un cadre conceptuel d√©velopp√© pour assurer la s√©curit√© tout au long 
du cycle de vie des syst√®mes d‚Äôintelligence artificielle, depuis la collecte des donn√©es jusqu‚Äôau d√©ploiement des mod√®les.
Il a √©t√© √©labor√© pour traiter les enjeux majeurs de gestion des risques, de s√©curit√© et de confidentialit√© propres aux 
mod√®les d‚ÄôIA et de machine learning. Le **SAIF** vise √† ce que ces syst√®mes soient ¬´ secure by default ¬ª d√®s 
leur mise en ≈ìuvre, en int√©grant des mesures de protection essentielles d√®s la phase de conception et tout au long du 
processus de d√©veloppement jusqu'au d√©ploiement en production.

**SAIF** s‚Äôinscrit dans la d√©marche g√©n√©rale de **Google** pour une IA responsable, align√©e sur des principes tels que: 
 - la s√©curit√©
 - l‚Äô√©quit√©
 - l‚Äôinterpr√©tabilit√©
 - la protection de la vie priv√©e. 

Le framework propose des normes et des 
contr√¥les concrets pour la construction, l‚Äô√©valuation et le d√©ploiement des syst√®mes d‚ÄôIA, en int√©grant la s√©curit√© de 
fa√ßon transverse.


### Les quatre grandes cat√©gories du SAIF

SAIF structure le d√©veloppement s√©curitaire de l‚ÄôIA autour de quatre axes majeurs, chacun √©tant associ√© √† plusieurs 
composants techniques cl√©s:

 - **Donn√©es** : concerne les sources de donn√©es, les processus de filtrage et de pr√©paration, ainsi que les ensembles 
utilis√©s pour l‚Äôentra√Ænement des mod√®les.
 - **Infrastructure** : englobe le mat√©riel, la s√©curit√© de l‚Äôh√©bergement, les frameworks et le code n√©cessaires √† 
l‚Äôentra√Ænement, le stockage des donn√©es et des mod√®les, ainsi que le d√©ploiement (Model Serving).
 - **Mod√®le** : inclut le mod√®le lui-m√™me (code et poids), la gestion des entr√©es (input handling) pour prot√©ger contre 
les entr√©es malicieuses, et la gestion des sorties (output handling) pour √©viter les expositions de donn√©es ou des 
comportements inattendus.
 - **Application** : se rapporte √† l‚Äôensemble applicatif qui interagit avec le mod√®le, y compris les agents ou plugins 
qui peuvent pr√©senter des risques additionnels.


### Les six √©l√©ments fondamentaux du SAIF

Le SAIF est articul√© autour de six √©l√©ments fondamentaux qui doivent √™tre continuellement mis en ≈ìuvre :

1. **√âtablir des bases de s√©curit√© solides pour l‚Äô√©cosyst√®me IA** : appliquer les principes traditionnels de s√©curit√© √† 
l‚Äôensemble de l‚Äô√©cosyst√®me IA, notamment pour l‚Äôinfrastructure, les codes sources et la cha√Æne d‚Äôapprovisionnement 
logicielle.
2. **√âtendre la d√©tection et la r√©ponse pour inclure l‚ÄôIA dans l‚Äôunivers des menaces de l‚Äôorganisation** : surveiller 
les entr√©es/sorties des syst√®mes IA et int√©grer l‚ÄôIA dans les dispositifs classiques de d√©tection et de r√©ponse aux 
incidents.
3. **Automatiser les d√©fenses pour suivre l‚Äô√©volution des menaces** : tirer parti des innovations IA pour contrer les 
attaques √† grande √©chelle et automatiser la protection.
4. **Harmoniser les contr√¥les au niveau des plateformes** : assurer une coh√©rence des politiques de s√©curit√© et des 
contr√¥les sur l‚Äôensemble des services et plateformes IA.
5. **Adapter les contr√¥les pour cr√©er des boucles de r√©troaction rapides** : ajuster et faire √©voluer rapidement les 
m√©canismes de s√©curit√© lors du d√©ploiement ou de l‚Äô√©volution des IA.
6. **Contextualiser les risques au sein des processus m√©tier** : int√©grer l‚Äôanalyse des risques IA dans la gestion 
globale des risques de l‚Äôorganisation


### Cartographie des risques et contr√¥les SAIF

SAIF donne acc√®s √† une cartographie d√©taill√©e des risques sp√©cifiques √† l‚ÄôIA, tels que :

- empoisonnement de donn√©es,
- utilisation de donn√©es non autoris√©es,
- alt√©ration du code ou des poids du mod√®le,
- exfiltration de mod√®les,
- attaque par d√©ni de service,
- divulgation ou inf√©rence de donn√©es sensibles, etc.

<a href="https://saif.google/secure-ai-framework/saif-map" target="_blank">
  <img src="img/saif-map.png" alt="SAIF">
</a>
<a href="https://saif.google/secure-ai-framework/saif-map" target="_blank"><em>source: saif.google</em></a>

√Ä chaque risque, des contr√¥les et mesures de mitigation sont propos√©s, ainsi que la d√©signation du responsable 
(cr√©ateur du mod√®le ou consommateur du mod√®le). Par exemple :

- Validation et assainissement des entr√©es/sorties
- Entra√Ænement et tests adversariaux
- Gestion des acc√®s, journalisation, surveillance continue

La cartographie SAIF permet d‚Äôidentifier o√π chaque risque √©merge (introduction), par o√π il peut √™tre exploit√© 
(exposition), et √† quel niveau il peut √™tre mitig√© (√©l√©ment ou contr√¥le associ√©).

### Mise en ≈ìuvre et communaut√© SAIF

**Google** met **SAIF** √† disposition comme point de r√©f√©rence pour les entreprises, gouvernements et organisations, avec un 
centre de ressources (saif.google) proposant des guides d‚Äôauto√©valuation, des contr√¥les de s√©curit√©, et un espace 
communautaire. Google travaille en coalition ([The Coalition for Secure AI](https://www.coalitionforsecureai.org/)) 
avec d‚Äôautres acteurs majeurs du secteur tels qu'**Amazon**, **Cisco**, **IBM**, **Intel**, **Microsoft**, **NVIDIA**, 
**OpenAI**,.. pour faire progresser l‚Äôadoption de ce framework, dans le but de s√©curiser l‚ÄôIA au b√©n√©fice de tous.

<details>
  <summary>The Coalition for Secure AI</summary>
<a href="https://www.coalitionforsecureai.org/" target="_blank">
  <img src="img/CoSAI.png" alt="image" width="450" style="transition:0.3s;">
</a>
</details>

## MITRE ATLAS, le fil d'Ariane des techniques d'attaque sur l'IA

<a href="https://www.riskinsight-wavestone.com/2024/11/lutilisation-pratique-du-cadre-atlas-de-mitre-pour-les-equipes-du-rssi/" target="_blank">
  <img src="https://www.riskinsight-wavestone.com/wp-content/uploads/2024/11/MITRE-Figure-1.png" alt="MITRE ATLAS" >
</a>
<a href="https://www.riskinsight-wavestone.com/2024/11/lutilisation-pratique-du-cadre-atlas-de-mitre-pour-les-equipes-du-rssi/" target="_blank"><em>source: riskinsight-wavestone.com</em></a>

### Objectif du MITRE ATLAS

Le **MITRE ATLAS** (Adversarial Tactics, Techniques, and Common Knowledge for AI Systems) est un cadre de r√©f√©rence 
international con√ßu pour identifier, classifier et att√©nuer les menaces adverses ciblant les syst√®mes d‚ÄôIA et 
d‚Äôapprentissage automatique. 

L‚Äôobjectif d‚ÄôATLAS est de fournir une base de connaissances structur√©e sur les tactiques 
et techniques employ√©es par les attaquants contre l‚ÄôIA, facilitant ainsi la protection proactive de ces syst√®mes et 
l‚Äôoptimisation de leur s√©curit√© dans l‚Äô√©cosyst√®me de l‚Äôentreprise.

### Cadre de r√©f√©rence

Le [**MITRE ATLAS**](https://atlas.mitre.org/) s‚Äôinspire fortement du c√©l√®bre cadre [**MITRE ATT&CK**](https://attack.mitre.org/), utilis√© dans la cybers√©curit√© traditionnelle pour 
cartographier les menaces et les actions adverses sur les syst√®mes d‚Äôinformation. ATLAS transpose cette d√©marche √† l‚ÄôIA, 
en se focalisant sur les risques, vuln√©rabilit√©s et techniques d‚Äôattaque sp√©cifiques aux technologies IA et machine 
learning.

### √âl√©ments fondamentaux du MITRE ATLAS

ATLAS est structur√© autour de plusieurs √©l√©ments cl√©s :

- **Tactiques** : Les objectifs de haut niveau poursuivis par les attaquants (ex. : √©vasion, compromission des donn√©es 
d‚Äôentra√Ænement, acc√®s initial).

- **Techniques** : Les m√©thodes concr√®tes permettant d‚Äôatteindre ces objectifs (ex. : data poisoning, model extraction, 
prompt injection).

- **Proc√©dures** : Exemples r√©els et cas d‚Äôutilisation illustrant comment ces techniques ont √©t√© appliqu√©es dans le 
monde r√©el.

- **√âtudes de cas** : Documentation d‚Äôattaques effectives sur des syst√®mes d‚ÄôIA pour enrichir continuellement la base 
de connaissances


### Comment l'utiliser
Le **MITRE ATLAS** est con√ßu pour √™tre utilis√© par les professionnels de la s√©curit√©, les chercheurs et les d√©veloppeurs. Il
faut voir **ATLAS** comme un tableau de bord structurant pour les √©quipes qui sont en charge de la s√©curit√© IA.

Les utilisateurs peuvent naviguer dans le r√©f√©rentiel pour identifier les menaces pertinentes √† leur contexte, comme:

- **Cartographie des menaces** : Les √©quipes de s√©curit√© mod√©lisent les risques et menaces qui p√®sent sur leur syst√®me d‚ÄôIA 
en utilisant la matrice ATLAS pour anticiper les techniques d‚Äôattaque possibles.
- **√âvaluation de la couverture d√©fensive** : √Ä l‚Äôaide d‚Äôoutils comme **ATLAS Navigator**, il est possible de visualiser 
quelles techniques d‚Äôattaque sont d√©j√† couvertes par les contr√¥les existants, lesquelles n√©cessitent des ajustements, 
ou pour lesquelles il faut cr√©er de nouveaux contr√¥les sp√©cifiques √† l‚ÄôIA.
- **D√©tection et r√©ponse aux incidents** : Lorsqu‚Äôun incident impliquant un syst√®me IA survient, ATLAS permet de tracer 
le d√©roul√© de l‚Äôattaque, d‚Äôidentifier les tactiques et techniques employ√©es, et de cibler la r√©ponse technique et 
organisationnelle.
- **Formation et sensibilisation** : **ATLAS** sert de support pour former les data scientists, ing√©nieurs IA et √©quipes 
SOC (Security Operations Center) aux menaces √©mergentes en IA, ouvrant la voie √† une collaboration interfonctionnelle √©troite entre m√©tiers et 
s√©curit√©.
- **D√©veloppement de cas d‚Äôusage d√©fensifs** : Les techniques document√©es dans ATLAS servent de base √† la cr√©ation de 
r√®gles de d√©tection, de sc√©narios de tests d‚Äôintrusion et de plans de mitigation sp√©cifiques aux syst√®mes d‚ÄôIA.


## R√©glementation l√©gislative des LLM

Entre 2013 et 2023, les entreprises am√©ricaines ont attir√© un volume de capitaux priv√©s plus de six fois sup√©rieur √† 
celui investi dans les entreprises europ√©ennes, favorisant ainsi l‚Äô√©mergence d‚Äôun √©cosyst√®me incomparable en mati√®re 
d‚Äôinnovation dans le domaine de l‚Äôintelligence artificielle.

√Ä titre de comparaison, les soci√©t√©s **am√©ricaines** ont lev√© environ **486,1 milliards de dollars** de financements priv√©s sur 
cette p√©riode, contre seulement **75,7 milliards** pour leurs homologues **europ√©ennes**.

<details>
  <summary>Comparatif graphique : 10 ans d‚Äôinvestissements en IA aux √âtats-Unis et dans l‚ÄôUnion europ√©enne.</summary>

<a href="https://actonline.org/2025/06/02/to-win-the-ai-race-congress-must-learn-from-europes-missteps/" target="_blank">
  <img src="https://actonline.org/wp-content/uploads/AI-blog-.png" alt="image" width="450" style="transition:0.3s;">
</a>
</details>

Ainsi, ces derni√®res ann√©es, de nombreux pays ont mis en place de nouvelles r√©gulations afin de faire face aux d√©rives 
li√©es aux technologies d‚Äôintelligence artificielle, notamment pour freiner la diffusion de la d√©sinformation et des 
discours de haine.
Voici un aper√ßu des approches adopt√©es par les √âtats-Unis et l‚ÄôUnion europ√©enne en mati√®re de r√©gulation des mod√®les de langage de grande taille (LLM).

### Enjeux et principes

- **√âquilibre responsabilit√©/innovation :** R√©guler les LLM exige de maintenir un juste √©quilibre entre la 
responsabilisation des acteurs (d√©veloppeurs, d√©ployeurs, utilisateurs) et la pr√©servation de l‚Äôinnovation. Les LLM 
pr√©sentent des avantages majeurs (√©ducation, accessibilit√©, cr√©ativit√©) mais comportent aussi des risques, comme la 
g√©n√©ration de contenus pr√©judiciables.


- **D√©finition de la responsabilit√© :** L‚Äôidentification des responsables des contenus g√©n√©r√©s par les LLM reste un 
enjeu. La responsabilit√© peut-elle incomber au d√©veloppeur, √† celui qui d√©ploie la solution, ou √† l‚Äôutilisateur ?


- **Respect des droits fondamentaux :** La lutte contre les abus ne doit pas compromettre des droits humains tels que la 
libert√© d‚Äôexpression. Les r√©glementations doivent donc viser la protection sans imposer une censure excessive.


## √âtats-Unis: une r√©gulation sectorielle et centr√©e sur la libert√© d‚Äôexpression

- **Libert√© d‚Äôexpression :** Aux √âtats-Unis, la diffusion d‚Äôinformations fausses ou controvers√©es, sauf en cas de 
diffamation, d‚Äôincitation √† la violence ou de fraude, reste prot√©g√©e par le premier amendement, rendant difficile 
l‚Äôinstauration de mesures de fond contre la d√©sinformation.


- **Take It Down Act :** Cette l√©gislation f√©d√©rale r√©cente cible la diffusion sur Internet de contenus abusifs, 
particuli√®rement les deepfakes et images intimes non consenties, y compris si elles sont g√©n√©r√©es par l‚ÄôIA. Elle impose 
aux plateformes un dispositif de signalement rapide et des obligations de retrait sous peine de sanctions administratives. 
La loi vient combler une lacune juridique majeure en mati√®re de deepfakes.


- **Obligations pour les plateformes :** Les r√©seaux sociaux et sites d‚Äôh√©bergement doivent retirer, dans un d√©lai court, 
les contenus signal√©s comme ill√©gaux. En cas de non-respect, ils s‚Äôexposent √† des sanctions importantes.  


- **Bonnes pratiques et autor√©gulation :** Les organismes publics comme le [NIST (National Institute of Standards and 
Technology)](https://www.nist.gov/) recommandent des pratiques de gestion des risques (AI RMF). D‚Äôautre part, 
la [Federal Trade Commission (FTC)](https://www.ftc.gov/) peut agir contre les pratiques commerciales trompeuses recourant √† l‚ÄôIA.


## Union europ√©enne: un encadrement structur√© et fond√© sur les risques
La r√©gulation europ√©enne s‚Äôarticule autour de deux textes principaux :

### Digital Services Act (DSA)

- **Obligations g√©n√©rales :** M√©canismes de signalement et suppression des contenus ill√©gaux ; droit de recours pour les 
utilisateurs en cas de suppression abusive.


- **Champ d‚Äôapplication large :** S‚Äôapplique √† tous les fournisseurs de services num√©riques op√©rant dans l‚ÄôUE, m√™me 
s‚Äôils sont bas√©s √† l‚Äô√©tranger.

- **√âvaluations de risques r√©currentes :** Pour certains acteurs majeurs, le DSA impose des √©valuations r√©guli√®res sur 
des probl√©matiques comme la d√©sinformation. Les plateformes doivent agir efficacement pour att√©nuer ces risques 
(modification d‚Äôalgorithmes, transparence accrue, etc.).


- **Renforcement de la transparence et des droits fondamentaux :** Protection contre les contenus nuisibles, respect de 
la vie priv√©e et de la libert√© d‚Äôexpression, publication claire des politiques de mod√©ration et de publicit√©.

### AI Act

- Une approche fond√©e sur 4 niveaux de risques :

  - **Risque inacceptable :** Syst√®mes interdits (ex. : social scoring, manipulation de masse).
  - **Risque √©lev√© :** Secteurs critiques (sant√©, √©ducation, s√©curit√©). Obligations strictes : gestion des risques, 
  gouvernance des donn√©es, supervision humaine.
  - **Risque limit√© :** Syst√®mes interagissant avec le public ou g√©n√©rant du contenu, comme les LLM. Exigences de 
  transparence (divulguer si un contenu est g√©n√©r√© par IA), documentation, mesures pour pr√©venir les usages abusifs.
  - **Risque minimal :** Applications simples telles que filtres anti-spam, largement non r√©glement√©es.


- **Innovations sp√©cifiques :** L‚ÄôAI Act pr√©voit des obligations particuli√®res pour les mod√®les d‚ÄôIA √† usage g√©n√©ral 
pr√©sentant un risque syst√©mique important (capacit√©s computationnelles massives).


- **P√©nalit√©s et contr√¥les :** Les contrevenants s‚Äôexposent √† des contr√¥les accrus et √† des sanctions financi√®res 
dissuasives.


### Points de convergence et divergences

Voici un tableau r√©capitulatif des diff√©rences et similitudes entre les approches r√©glementaires des √âtats-Unis et 
de l‚ÄôUnion europ√©enne concernant les LLM :

| Points                            | √âtats-Unis                                                                                            | Union europ√©enne                                                                                             |
|-----------------------------------|-------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|
| **Priorit√© fondamentale**         | Libert√© d‚Äôexpression pr√©serv√©e, interventions cibl√©es sur des enjeux pr√©cis (deepfakes, fraude).      | Approche globale couvrant l‚Äôensemble du cycle de vie de l‚ÄôIA selon le niveau de risque .                     |
| **Philosophie de r√©gulation**     | Focalis√©e sur les cas particuliers et d√©fense du Premier Amendement.                                  | Pr√©ventive, syst√©mique, avec exigence de transparence, documentation et gestion du risque √† chaque √©tape .   |
| **Responsabilisation**            | Obligations cibl√©es sur certaines plateformes ; sanctions et normes variables selon les √âtats et cas. | Responsabilisation claire et harmonis√©e pour plateformes et fournisseurs, r√©gime de sanction structur√© .     |
| **Transparence et documentation** | Exigences limit√©es √† certains cas sp√©cifiques (deepfakes, fraude), pas de cadre g√©n√©ral harmonis√©.    | Forte exigence de transparence, documentation, acc√®s public √† l‚Äôinformation pour syst√®mes IA √† risque .      |
| **Harmonisation des sanctions**   | Fragment√©e, grandes variations selon √âtats f√©d√©r√©s et interventions f√©d√©rales ponctuelles.            | Sanctions harmonis√©es, r√©guli√®rement renforc√©es et uniformis√©es au niveau de l‚ÄôUE.                           |

Cette opposition illustre la diversit√© des strat√©gies de r√©gulation, oscillant entre respect des libert√©s et pr√©vention 
efficace des abus technologiques.


## Ressources

| Information                                                                                          | Lien                                                                                                                                                                                                                                                                                                                                                         |
|------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| OWASP Top 10 for LLM Applications                                                                    | [https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf](https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf)                                                                                                       |
| Explained: The OWASP Top 10 for Large Language Model Applications                                    | [https://www.youtube.com/watch?v=cYuesqIKf9A](https://www.youtube.com/watch?v=cYuesqIKf9A)                                                                                                                                                                                                                                                                   |
| Model theft: Meta LLaMA leak                                                                         | [https://learn.snyk.io/lesson/model-theft-llm/](https://learn.snyk.io/lesson/model-theft-llm/)                                                                                                                                                                                                                                                               |
| Secure AI Framework (SAIF)                                                                           | [https://saif.google/](https://saif.google/)                                                                                                                                                                                                                                                                                                                 |
| Google lance un framework pour s√©curiser les mod√®les d'IA                                            | [https://www.lemondeinformatique.fr/actualites/lire-google-lance-un-framework-pour-securiser-les-modeles-d-ia-90694.html](https://www.lemondeinformatique.fr/actualites/lire-google-lance-un-framework-pour-securiser-les-modeles-d-ia-90694.html)                                                                                                           |
| Pr√©sentation de la s√©curit√© dans le monde de l'IA                                                    | [https://www.cloudskillsboost.google/paths/1283/course_templates/1147?locale=fr](https://www.cloudskillsboost.google/paths/1283/course_templates/1147?locale=fr)                                                                                                                                                                                             |
| What Is Google's Secure AI Framework (SAIF)?                                                         | [https://www.paloaltonetworks.com/cyberpedia/google-secure-ai-framework](https://www.paloaltonetworks.com/cyberpedia/google-secure-ai-framework)                                                                                                                                                                                                             |
| Secure AI Framework Approach                                                                         | [https://kstatic.googleusercontent.com/files/00e270b1cccb1f37302462a162c171d86f293a84de54036e0021e2fe0253cf05623bae2a62751b0840667bc6c8412fd70f45c9485972dc370be8394fae922d31](https://kstatic.googleusercontent.com/files/00e270b1cccb1f37302462a162c171d86f293a84de54036e0021e2fe0253cf05623bae2a62751b0840667bc6c8412fd70f45c9485972dc370be8394fae922d31) |
| Securing the AI Pipeline                                                                             | [https://cloud.google.com/blog/topics/threat-intelligence/securing-ai-pipeline/?hl=en](https://cloud.google.com/blog/topics/threat-intelligence/securing-ai-pipeline/?hl=en)                                                                                                                                                                                 |
| Introducing the Coalition for Secure AI (CoSAI) and founding member organizations                    | [https://blog.google/technology/safety-security/google-coalition-for-secure-ai/](https://blog.google/technology/safety-security/google-coalition-for-secure-ai/)                                                                                                                                                                                             |
| MITRE ATLAS                                                                                          | [https://atlas.mitre.org/](https://atlas.mitre.org/)                                                                                                                                                                                                                                                                                                         |
| Anatomy of an AI ATTACK: MITRE ATLAS                                                                 | [https://www.youtube.com/watch?v=QhoG74PDFyc](https://www.youtube.com/watch?v=QhoG74PDFyc)                                                                                                                                                                                                                                                                   |
| MITRE ATLAS‚Ñ¢ Introduction                                                                            | [https://www.youtube.com/watch?v=3FN9v-y-C-w](https://www.youtube.com/watch?v=3FN9v-y-C-w)                                                                                                                                                                                                                                                                   |
| L‚Äôutilisation pratique du cadre ATLAS de MITRE pour les √©quipes du RSSI                              | [https://www.riskinsight-wavestone.com/2024/11/lutilisation-pratique-du-cadre-atlas-de-mitre-pour-les-equipes-du-rssi/](https://www.riskinsight-wavestone.com/2024/11/lutilisation-pratique-du-cadre-atlas-de-mitre-pour-les-equipes-du-rssi/)                                                                                                               |
| Introduction √† l'apprentissage automatique contradictoire et au cadre MITRE ATLAS                    | [https://www.infosecured.ai/fr/i/mlsec/adversarial-machine-learning-mitre-atlas-framework/](https://www.infosecured.ai/fr/i/mlsec/adversarial-machine-learning-mitre-atlas-framework/)                                                                                                                                                                       |
| MITRE ATLAS Framework 2025 ‚Äì Guide to Securing AI Systems                                            | [https://www.practical-devsecops.com/mitre-atlas-framework-guide-securing-ai-systems/](https://www.practical-devsecops.com/mitre-atlas-framework-guide-securing-ai-systems/)                                                                                                                                                                                 |
| ATLAS Navigator                                                                                      | [https://github.com/mitre-atlas/atlas-navigator](https://github.com/mitre-atlas/atlas-navigator)                                                                                                                                                                                                                                                             |
| Faut-il investir sur la tech europ√©enne ? L'analyse d'un insider - Finary Talk #60 & Olivier Coste - | [https://youtu.be/Tw-HRXlVIa0?si=ZRHWRjy_vzHcQ6Af](https://youtu.be/Tw-HRXlVIa0?si=ZRHWRjy_vzHcQ6Af)                                                                                                                                                                                                                                                         |
| AI Act                                                                                               | [https://artificialintelligenceact.eu/fr/](https://artificialintelligenceact.eu/fr/)                                                                                                                                                                                                                                                                         |
| AI Act - European Commission                                                                         | [https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai)                                                                                                                                                                                                       |
| Digital Services Act (DSA)                                                                           | [https://www.vie-publique.fr/eclairage/285115-dsa-le-reglement-sur-les-services-numeriques-ou-digital-services-act](https://www.vie-publique.fr/eclairage/285115-dsa-le-reglement-sur-les-services-numeriques-ou-digital-services-act)                                                                                                                       |
| Take It Down Act                                                                                     | [https://www.congress.gov/bill/119th-congress/senate-bill/146](https://www.congress.gov/bill/119th-congress/senate-bill/146)                                                                                                                                                                                                                                 |
| NIST AI RMF                                                                                          | [https://www.nist.gov/itl/ai-risk-management-framework](https://www.nist.gov/itl/ai-risk-management-framework)                                                                                                                                                                                                                                               |
| Artificial Intelligence Regulation Threatens Free Expression                                         | [https://www.cato.org/briefing-paper/artificial-intelligence-regulation-threatens-free-expression](https://www.cato.org/briefing-paper/artificial-intelligence-regulation-threatens-free-expression)                                                                                                                                                         |
| AI regulation: EU vs. USA - opportunities and challenges for companies                               | [https://www.tucan.ai/blog/ai-regulation-eu-vs-usa-opportunities-and-challenges-for-companies/](https://www.tucan.ai/blog/ai-regulation-eu-vs-usa-opportunities-and-challenges-for-companies/)                                                                                                                                                               |
| Comparing the EU AI Act to Proposed AI-Related Legislation in the US                                 | [https://businesslawreview.uchicago.edu/online-archive/comparing-eu-ai-act-proposed-ai-related-legislation-us](https://businesslawreview.uchicago.edu/online-archive/comparing-eu-ai-act-proposed-ai-related-legislation-us)                                                                                                                                 |
| Take It Down Act : la loi contre le revenge porn et les deepfakes pornographiques.                   | [https://toutsurlacyber.fr/take-it-down-act-revenge-porn-et-deepfakes/](https://toutsurlacyber.fr/take-it-down-act-revenge-porn-et-deepfakes/)                                                                                                                                                                                                               |
| Le minist√®re am√©ricain du Commerce publie un outil de mesure des risques inh√©rents aux LLM           | [https://www.usine-digitale.fr/article/le-ministere-americain-du-commerce-publie-un-outil-de-mesure-des-risques-inherents-aux-llm.N2216752](https://www.usine-digitale.fr/article/le-ministere-americain-du-commerce-publie-un-outil-de-mesure-des-risques-inherents-aux-llm.N2216752)                                                                       |
| Entr√©e en vigueur du r√®glement europ√©en sur l‚ÄôIA : les premi√®res questions-r√©ponses de la CNIL       | [https://www.cnil.fr/fr/entree-en-vigueur-du-reglement-europeen-sur-lia-les-premieres-questions-reponses-de-la-cnil](https://www.cnil.fr/fr/entree-en-vigueur-du-reglement-europeen-sur-lia-les-premieres-questions-reponses-de-la-cnil)                                                                                                                     |
| GenAI Incident Response Guide                                                                        | [https://genai.owasp.org/resource/genai-incident-response-guide-1-0/](https://genai.owasp.org/resource/genai-incident-response-guide-1-0/)                                                                                                                                                                                                                   |
