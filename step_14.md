#  AI Red Teaming

[<img src="img/step15.jpg" alt="gandalf" width="800">](https://www.youtube.com/watch?v=lKSKJZ-XdAk)
> "Fight for us... and regain your honor", Aragorn, LOTR - The Return of the King


## üéØ Objectifs de cette √©tape

- Comprendre les principes du red teaming appliqu√© aux IA g√©n√©ratives
- Identifier les vuln√©rabilit√©s sp√©cifiques aux syst√®mes d‚ÄôIA g√©n√©rative
- Mettre en ≈ìuvre des techniques d‚Äôattaque et d‚Äô√©valuation adapt√©es
- Proposer des strat√©gies de mitigation et de d√©fense

## Sommaire

- [Red Teaming des intelligences artificielles g√©n√©ratives](#red-teaming-des-intelligences-artificielles-g√©n√©ratives)

- [Approche face aux IA g√©n√©ratives](#approche-face-aux-ia-g√©n√©ratives)
    - [La bo√Æte noire](#la-bo√Æte-noire)
    - [D√©pendance aux donn√©es](#d√©pendance-aux-donn√©es)
    - [Composants des syst√®mes d‚ÄôIA g√©n√©rative](#composants-des-syst√®mes-dia-g√©n√©rative)

- [Techniques, tactiques et proc√©dures (TTP) sp√©cifiques](#techniques-tactiques-et-proc√©dures-ttp-sp√©cifiques)
- [La recette du succ√®s](#la-recette-du-succ√®s)

- [√âtape suivante](#√©tape-suivante)
- [Ressources](#ressources)

## Red Teaming des intelligences artificielles g√©n√©ratives

Le red teaming des IA g√©n√©ratives ne doit pas se limiter √† la recherche de failles techniques isol√©es au niveau des 
mod√®les. Il s'agit d'une d√©marche critique et syst√©matique qui doit couvrir l'ensemble du cycle de vie du syst√®me d'IA, 
depuis la conception jusqu'au d√©ploiement et l'exploitation, en tenant compte des interactions complexes entre les 
mod√®les, les utilisateurs et l'environnement op√©rationnel.


Cette vision holistique n√©cessite la constitution d'√©quipes multifonctionnelles qui combinent comp√©tences techniques, 
socioculturelles, √©thiques et l√©gales afin d'identifier les risques √©mergents, les vuln√©rabilit√©s syst√©miques et les 
comportements impr√©vus qui peuvent survenir dans les environnements r√©els.


Le red teaming comme exercice de pens√©e critique d√©passe la simple chasse √† la faille pour int√©grer l'analyse des 
risques sociotechniques, y compris les risques li√©s aux interactions humaines et aux usages d√©tourn√©s.

## Approche face aux IA g√©n√©ratives

L'approche d‚Äô√©valuation doit √™tre agile et adaptative, s‚Äôappuyant sur une compr√©hension de l‚Äô√©volution constante des 
syst√®mes. Il est essentiel de surveiller et d‚Äôint√©grer les informations du d√©veloppement √† la production, en incluant 
un contr√¥le rigoureux et continu des donn√©es, de la conception, du code, des environnements d‚Äôentra√Ænement, jusqu‚Äôaux 
comportements en conditions r√©elles.

L'√©valuation doit √™tre men√©e √† plusieurs niveaux, combinant les tests sur les mod√®les eux-m√™mes (micro-niveau) et sur 
le syst√®me global incluant ses interfaces, ses composants techniques et humains (macro-niveau).

### La bo√Æte noire

La nature "bo√Æte noire" des mod√®les n√©cessite des approches sp√©cifiques :

- Tester les mod√®les en bo√Æte noire tout en exploitant, lorsque possible, des versions open source en environnements 
contr√¥l√©s pour recr√©er et comprendre les failles.
- Examiner la conception et l'architecture du syst√®me pour identifier les vuln√©rabilit√©s syst√©mique en amont, notamment 
dans les interfaces, les m√©canismes d'observabilit√©, et les syst√®mes de gouvernance int√©gr√©s d√®s la conception.

Cela inclut d'anticiper les modes de d√©faillance li√©s √† des cas extr√™mes, des interactions inattendues entre composants, 
ou la manipulation des flux de donn√©es.

### D√©pendance aux donn√©es

La qualit√© et la gestion des donn√©es sont des facteurs cruciaux. Le red teaming doit √©valuer toute la cha√Æne de donn√©e :

- Collecte, stockage, acc√®s et tra√ßabilit√©,
- Repr√©sentativit√© et biais des donn√©es,
- Protection contre les attaques comme l'empoisonnement de donn√©es (data poisoning),
- Risques li√©s √† la d√©-anonymisation ou √† la combinaison de donn√©es apparemment innocentes.

Le contr√¥le rigoureux des pipelines de donn√©es et la prise en compte des √©volutions (d√©rives, biais, attaques) sont 
essentiels.

### Composants des syst√®mes d‚ÄôIA g√©n√©rative

Les vuln√©rabilit√©s doivent √™tre analys√©es dans l‚Äôensemble des composants critiques :

| Composant   | Description des vuln√©rabilit√©s sp√©cifiques                                                             |
|-------------|--------------------------------------------------------------------------------------------------------|
| Mod√®le      | Failles algorithmiques, injections de prompt, erreurs d'optimisation ou biais exploitable.             |
| Donn√©es     | Qualit√©, provenance, biais, attaques par empoisonnement, gestion des acc√®s et vie des donn√©es.         |
| Application | Failles classiques de s√©curit√© applicative pouvant √™tre vecteur d'attaque vers le mod√®le.              |
| Syst√®me     | Infrastructures, r√©seaux, outils de d√©veloppement, cha√Ænes d‚Äôapprovisionnement pouvant √™tre compromis. |

Le red teaming doit inclure l'√©valuation des environnements de d√©veloppement, pipelines de d√©ploiement, contr√¥les 
d'acc√®s et la r√©silience du syst√®me face aux d√©faillances.

### Techniques, tactiques et proc√©dures (TTP) sp√©cifiques

Outre les TTP classiques issues de la cybers√©curit√© (phishing, exploitation de vuln√©rabilit√©s, mouvements lat√©raux, 
exfiltration), les campagnes de red teaming doivent int√©grer :

- L'injection et manipulation dans les prompts (red teaming mod√®le),
- Les attaques sur la cha√Æne d'approvisionnement des donn√©es et des composants,
- Les sc√©narios d'attaques sociotechniques li√©es aux comportements utilisateurs et aux interactions humaines,
- La prise en compte des divers contextes culturels, linguistiques et d√©mographiques pour une √©valuation exhaustive
et repr√©sentative.

### La recette du succ√®s

Le succ√®s du red teaming repose sur une collaboration entre disciplines techniques et non techniques, visant √† embrasser
la complexit√© des syst√®mes d‚ÄôIA. Il s'agit de combiner red teaming (identification des risques) avec des m√©thodes 
compl√©mentaires comme le violet teaming qui combine avec le blue teaming (r√©ponses et rem√©diations).

La diversit√© des √©quipes (comp√©tences, perspectives utilisateur, aspects culturels) est essentielle pour d√©tecter des 
risques invisibles aux approches homog√®nes.


## √âtape suivante

- [√âtape 15](step_15.md)

## Ressources

| Information                                                   | Lien                                                                                                                                                                                   |
|---------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| NVIDIA AI Red Team: An Introduction                           | [https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/](https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/)                                         |
| Defining LLM Red Teaming                                      | [https://developer.nvidia.com/blog/defining-llm-red-teaming/](https://developer.nvidia.com/blog/defining-llm-red-teaming/)                                                             |
| Red Teaming AI Red Teaming                                    | [https://arxiv.org/abs/2507.05538](https://arxiv.org/abs/2507.05538)                                                                                                                   |
| Red Teaming AI: Attacking & Defending Intelligent Systems     | [https://www.amazon.com.au/Red-Teaming-Attacking-Defending-Intelligent-ebook/dp/B0F88SGMXG](https://www.amazon.com.au/Red-Teaming-Attacking-Defending-Intelligent-ebook/dp/B0F88SGMXG) |
| SAFE-AI A Framework for Securing AI-Enabled Systems           | [https://www.linkedin.com/feed/update/urn:li:activity:7346561112877821953/](https://www.linkedin.com/feed/update/urn:li:activity:7346561112877821953/)                                 |
| RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning | [https://arxiv.org/abs/2510.06994](https://arxiv.org/abs/2510.06994)                                                                                                                   |
| Prompt Injection Taxonomy                                     | [https://github.com/Arcanum-Sec/arc_pi_taxonomy/](https://github.com/Arcanum-Sec/arc_pi_taxonomy/)                                                                                     |
