#  AI Red Teaming

[<img src="img/step15.jpg" alt="gandalf" width="800">](https://www.youtube.com/watch?v=lKSKJZ-XdAk)
> "Fight for us... and regain your honor", Aragorn, LOTR - The Return of the King


## ðŸŽ¯ Objectifs de cette Ã©tape

- Comprendre les principes du red teaming appliquÃ© aux IA gÃ©nÃ©ratives
- Identifier les vulnÃ©rabilitÃ©s spÃ©cifiques aux systÃ¨mes dâ€™IA gÃ©nÃ©rative
- Mettre en Å“uvre des techniques dâ€™attaque et dâ€™Ã©valuation adaptÃ©es
- Proposer des stratÃ©gies de mitigation et de dÃ©fense

## Sommaire

 - [Red Teaming des intelligences artificielles gÃ©nÃ©ratives](#red-teaming-des-intelligences-artificielles-gÃ©nÃ©ratives)

 - [Approche face aux IA gÃ©nÃ©ratives](#approche-face-aux-ia-gÃ©nÃ©ratives)
   - [La boÃ®te noire](#la-boÃ®te-noire)
   - [DÃ©pendance aux donnÃ©es](#dÃ©pendance-aux-donnÃ©es)
   - [Composants des systÃ¨mes dâ€™IA gÃ©nÃ©rative](#composants-des-systÃ¨mes-dâ€™ia-gÃ©nÃ©rative)

 - [Techniques, tactiques et procÃ©dures (TTP) spÃ©cifiques](#techniques-tactiques-et-procÃ©dures-ttp-spÃ©cifiques)

 - [Ã‰tape suivante](#Ã©tape-suivante)
 - [Ressources](#ressources)

## Red Teaming des intelligences artificielles gÃ©nÃ©ratives

Le red teaming des intelligences artificielles gÃ©nÃ©ratives est une approche proactive visant Ã  identifier les 
vulnÃ©rabilitÃ©s et les faiblesses des systÃ¨mes d'IA gÃ©nÃ©rative. 

Cependant, le red teaming prÃ©sentent des complexitÃ©s et des subtilitÃ©s particuliÃ¨res. Avec lâ€™explosion rÃ©cente des modÃ¨les 
dâ€™apprentissage automatique (ML) et la multiplication des dÃ©ploiements de systÃ¨mes basÃ©s sur ces techniques, 
le domaine Ã©volue de maniÃ¨re dynamique et adaptative. 

Cette Ã©volution perpÃ©tuelle engendre des dÃ©fis spÃ©cifiques pour les administrateurs et les testeurs dâ€™intrusion, 
notamment en raison du risque accru de mauvaise configuration ou de failles lors du dÃ©ploiement des modÃ¨les, pouvant 
engendrer des vulnÃ©rabilitÃ©s de sÃ©curitÃ©.


## Approche face aux IA gÃ©nÃ©ratives

Lorsquâ€™on Ã©value la sÃ©curitÃ© des systÃ¨mes reposant sur des IA gÃ©nÃ©ratives, il est essentiel de comprendre leur nature 
adaptative et Ã©volutive. Pour cela, il faut rester informÃ©Â·e des avancÃ©es dans ce domaine afin dâ€™identifier en temps 
rÃ©el les failles potentielles. 

En outre, une approche dâ€™Ã©valuation de sÃ©curitÃ© agile et crÃ©ative est nÃ©cessaire pour exploiter ces vulnÃ©rabilitÃ©s, 
surtout lorsquâ€™elles sont protÃ©gÃ©es par des mÃ©canismes de mitigation spÃ©cifiques.

### La boÃ®te noire

Un des grands dÃ©fis inhÃ©rents aux modÃ¨les complexes utilisÃ©s en IA gÃ©nÃ©rative est leur cÃ´tÃ© Â« boÃ®te noire Â» : 
il est souvent difficile de comprendre pourquoi un modÃ¨le rÃ©agit dâ€™une certaine maniÃ¨re Ã  une entrÃ©e donnÃ©e, et encore 
plus complexe de prÃ©dire sa rÃ©action Ã  de nouvelles entrÃ©es. 

Par consÃ©quent, les Ã©valuations de sÃ©curitÃ© doivent adop-ter une posture de test en boÃ®te noire, mÃªme lorsque le type 
de modÃ¨le est connu. Cela implique de dÃ©velopper des stratÃ©gies dâ€™attaque innovantes pour dÃ©celer et exploiter les 
vulnÃ©rabilitÃ©s.

Cependant, la connaissance du type de modÃ¨le utilisÃ© peut faciliter ces Ã©valuations. Par exemple, si le modÃ¨le cible 
est open source, on peut le tÃ©lÃ©charger et lâ€™hÃ©berger soi-mÃªme, ce qui permet de tester les vulnÃ©rabilitÃ©s courantes 
en toute sÃ©curitÃ©, sans perturber le service ou susciter de suspicion. Cette approche peut aussi accÃ©lÃ©rer les tests 
face aux protections classiques comme la limitation de dÃ©bit (rate limiting).


--> TODO

## Ã‰tape suivante

- [Ã‰tape 15](step_15.md)

## Ressources

| Information                                               | Lien                                                                                                                                                                                   |
|-----------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Red Teaming AI Red Teaming                                | [https://arxiv.org/abs/2507.05538](https://arxiv.org/abs/2507.05538)                                                                                                                   |
| NVIDIA AI Red Team: An Introduction                       | [https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/](https://developer.nvidia.com/blog/nvidia-ai-red-team-an-introduction/)                                         |
| Defining LLM Red Teaming                                  | [https://developer.nvidia.com/blog/defining-llm-red-teaming/](https://developer.nvidia.com/blog/defining-llm-red-teaming/)                                                             |
| Red Teaming AI: Attacking & Defending Intelligent Systems | [https://www.amazon.com.au/Red-Teaming-Attacking-Defending-Intelligent-ebook/dp/B0F88SGMXG](https://www.amazon.com.au/Red-Teaming-Attacking-Defending-Intelligent-ebook/dp/B0F88SGMXG) |
| SAFE-AI A Framework for Securing AI-Enabled Systems       | [https://www.linkedin.com/feed/update/urn:li:activity:7346561112877821953/](https://www.linkedin.com/feed/update/urn:li:activity:7346561112877821953/)                                 |
