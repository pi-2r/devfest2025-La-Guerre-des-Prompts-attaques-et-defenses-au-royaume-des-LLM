# Test de robustesse ?

[<img src="img/step8.png" alt="hobbiton" width="800">](https://www.youtube.com/watch?v=NFBXilomkPk)
> "Oh... you would not part an old man from his walking stick?", Gandalf, LOTR - The Two Towers

## üéØ Objectifs de cette √©tape

- Comprendre l'importance des tests de robustesse pour la s√©curit√© des mod√®les d'IA.
- Identifier les principales menaces adversariales (empoisonnement, √©vasion) et leurs impacts.
- D√©couvrir les m√©thodes courantes pour tester la robustesse des mod√®les.
- Explorer l'int√©gration des tests de robustesse dans le cycle de vie de l'IA (MLOps, CI/CD, red teaming).
- Comparer les outils Garak et PyRIT pour le red teaming et la robustesse des LLM.


## Sommaire

- [Introduction](#introduction)
- [Objectif principal](#objectif-principal)


- [Contexte des Attaques cibl√©es](#contexte-des-attaques-cibl√©es)
- [M√©thodes](#m√©thodes)
- [Int√©gration dans le Cycle de Vie de l'IA](#int√©gration-dans-le-cycle-de-vie-de-lia)


- [AI Red Teaming: Garak vs PyRIT](#ai-red-teaming-garak-vs-pyrit)
  - [Garak](#garak)
  - [PyRIT](#pyrit)
  - [Tableau comparatif](#tableau-comparatif)
  


- [√âtape suivante](#√©tape-suivante)
- [Ressources](#ressources)


## Introduction

Les **tests de robustesse**, √©galement appel√©s **tests adversariaux**, repr√©sentent un √©l√©ment cl√© de la s√©curit√© des syst√®mes 
d‚Äôintelligence artificielle (IA) et d‚Äôapprentissage automatique (ML). Ils visent avant tout **√† mesurer la capacit√© d‚Äôun 
mod√®le √† rester fiable et performant** lorsqu‚Äôil est confront√© √† des donn√©es volontairement con√ßues pour l‚Äôinduire en 
erreur ou le manipuler.


## Objectif principal

L'objectif principal des tests de robustesse ont pour but d‚Äô√©valuer la capacit√© d‚Äôun mod√®le √† pr√©server ses 
performances et son int√©grit√© face √† des entr√©es malveillantes ou alt√©r√©es de mani√®re subtile. Ils cherchent √† s‚Äôassurer
que le mod√®le conserve un comportement conforme aux attentes et ne g√©n√®re pas de r√©sultats erron√©s ou potentiellement
dangereux lorsqu‚Äôil est soumis √† des attaques.

## Contexte des Attaques cibl√©es

Ces tests sont cruciaux pour se d√©fendre contre deux cat√©gories principales d'attaques adversariales :

- **Attaques par Empoisonnement des Donn√©es (Poisoning Attacks)** Ces attaques ciblent la phase d'entra√Ænement du 
mod√®le. Les adversaires manipulent subtilement les donn√©es d'entra√Ænement pour compromettre le processus d'apprentissage
et influencer malicieusement les r√©sultats du mod√®le lors de l'inf√©rence. Les tests de robustesse √©valuent la 
susceptibilit√© d'un mod√®le aux backdoors (portes d√©rob√©es) ins√©r√©es via l'empoisonnement et peuvent aider √† d√©tecter 
la pr√©sence de donn√©es alt√©r√©es.


- **Attaques par √âvasion (Evasion Attacks)**: Ces attaques se produisent pendant la phase d'inf√©rence sur un mod√®le 
d√©j√† entra√Æn√©. Les attaquants cr√©ent des "exemples adversariaux" ‚Äì des entr√©es qui ont √©t√© l√©g√®rement modifi√©es 
(souvent de mani√®re imperceptible pour un humain) ‚Äì pour provoquer une classification incorrecte par le mod√®le. 
Les tests de robustesse v√©rifient la capacit√© du mod√®le √† r√©sister √† ces tentatives de manipulation.

## M√©thodes

Quelques apparoches courantes pour r√©aliser des tests de robustesse incluent :

- **Tests personnalis√©s** : Ils sont d√©velopp√©s pour √©valuer la d√©fense contre des types sp√©cifiques d'attaques
par empoisonnement.

- **Enregistrements "Canary"** : Une m√©thode simple pour d√©tecter l'empoisonnement des donn√©es, consistant √† inclure 
une petite s√©lection d'enregistrements clairement identifiables. Si ces enregistrements sont mal classifi√©s, cela peut 
signaler un empoisonnement. Cependant, cette m√©thode ne d√©tecte pas les backdoors.

- **Entra√Ænement Adversarial** : Cette technique consiste √† fortifier le mod√®le en incorporant des exemples 
adversariaux (normaux et malveillants) dans son jeu de donn√©es d'entra√Ænement. L'objectif est d'enseigner au 
mod√®le √† identifier et √† neutraliser de mani√®re autonome les entr√©es nuisibles.


## Int√©gration dans le Cycle de Vie de l'IA
Les tests de robustesse sont des activit√©s cl√©s dans la s√©curit√© de l'IA et sont int√©gr√©s √† plusieurs niveaux :

- **MLOps et pipelines CI/CD** : Ils sont de plus en plus int√©gr√©s dans les pipelines MLOps et CI/CD pour automatiser 
l'√©valuation des mod√®les, en particulier apr√®s le fine-tuning ou la mise √† jour.

- **√âvaluation des mod√®les tiers** : Lors de l'acquisition de mod√®les pr√©-entra√Æn√©s, ces tests sont essentiels pour √©
valuer leur int√©grit√© et d√©tecter d'√©ventuels empoisonnements ou alt√©rations.

- **Red Teaming** : Les tests de robustesse sont une composante fondamentale du "red teaming" en IA, une approche 
offensive qui simule des cyberattaques pour identifier de mani√®re proactive les vuln√©rabilit√©s avant le d√©ploiement 
du mod√®le.

- **Benchmarking** : Ils compl√®tent les benchmarks de s√©curit√© et d'√©thique pour les LLM (comme DecodingTrust) en 
fournissant une √©valuation sp√©cifique de la r√©sistance aux attaques.

## AI Red Teaming: Garak vs PyRIT
Deux outils open source populaires pour effectuer des tests de robustesse sur les mod√®les d'IA g√©n√©rative 
sont **Garak** et **PyRIT**.

**Garak** repose sur une biblioth√®que d‚Äôattaques d√©j√† connues qu‚Äôil lance pour tester, tandis que **PyRIT** permet de 
personnaliser, cha√Æner et automatiser des sc√©narios d‚Äôattaque complexes selon la politique de s√©curit√© souhait√©e.

### Garak

**Garak** est un framework d√©velopp√© par **NVIDIA**, pour mener des tests adversariaux sur les mod√®les de langage (LLM). 
En quelques points:

- Il g√©n√®re automatiquement des entr√©es pi√©geuses (prompts adversariaux) pour d√©tecter les vuln√©rabilit√©s, comme 
la propagation de contenus biais√©s, dangereux ou mensongers.

- Il est muni d‚Äôune biblioth√®que tr√®s large d‚Äôattaques connues, dont des jail-breaks, fuites, contournements de filtres
et plusieurs variantes.

- Garak se concentre principalement sur les attaques "one-shot" et l‚Äôanalyse statique, en documentant explicitement 
chaque prompt et r√©sultat.

### PyRIT

**PyRIT (Python Risk Identification Tool)** est aussi un outil open source, d√©velopp√© par **Microsoft**, sp√©cialis√© dans 
l‚Äôautomatisation du ¬´ red teaming ¬ª pour tester la s√©curit√© des mod√®les g√©n√©ratifs d‚ÄôIA.
En quelques points:

- Il permet de cr√©er et personnaliser des s√©quences d‚Äôattaques selon des politiques ou des sc√©narios de risque 
sp√©cifiques, avec une architecture modulaire qui s‚Äôadapte √† diff√©rents contextes et risques.

- PyRIT facilite les tests structur√©s, le chainage d‚Äô√©tapes et la simulation de comportements d‚Äôattaque complexes et 
longs, contrairement √† Garak qui attaque principalement en "one-shot"

### Tableau comparatif

| Outil  | Approche principale                |  Atouts            |  Personnalisation    | Focus technique                              |                     
|--------|------------------------------------|--------------------|----------------------|----------------------------------------------|
| Garak  | Biblioth√®que d‚Äôattaques            |  Large couverture  |  Limit√©              | Attaques one-shot, analyse statique          |    
| PyRIT  | G√©n√©ration par sc√©narios/policy    |  Attaques cibl√©es  |  √âlev√©e (templates)  | S√©quences d‚Äôattaques, architecture modulaire |


## √âtape suivante
- [√âtape 9](step_9.md)

## Ressources


| Information                                                                                    | Lien                                                                                                                                                                                                                                 |
|------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Promptfoo vs Deepteam vs PyRIT vs Garak: The Ultimate Red Teaming Showdown for LLMs            | [https://dev.to/ayush7614/promptfoo-vs-deepteam-vs-pyrit-vs-garak-the-ultimate-red-teaming-showdown-for-llms-48if](https://dev.to/ayush7614/promptfoo-vs-deepteam-vs-pyrit-vs-garak-the-ultimate-red-teaming-showdown-for-llms-48if) |
| AI Security in Action: Applying NVIDIA‚Äôs Garak to LLMs on Databricks                           | [https://www.databricks.com/blog/ai-security-action-applying-nvidias-garak-llms-databricks](https://www.databricks.com/blog/ai-security-action-applying-nvidias-garak-llms-databricks)                                               |
| Garak: A Framework for Security Probing Large Language Models                                  | [https://arxiv.org/abs/2407.13499](https://arxiv.org/abs/2407.13499)                                                                                                                                                                 |
| PyRIT: Framework for Security Risk Identification and Red Teaming in Generative AI System      | [https://arxiv.org/abs/2407.13498](https://arxiv.org/abs/2407.13498)                                                                                                                                                                 |
| AI‚Äôs Achilles‚Äô Heel: Identifying and Securing Against Hidden Risks                             | [https://techstrong.ai/building-with-ai/ais-achilles-heel-identifying-and-securing-against-hidden-risks/](https://techstrong.ai/building-with-ai/ais-achilles-heel-identifying-and-securing-against-hidden-risks/)                   |                                                              | [
| Insights and Current Gaps in Open-Source LLM Vulnerability Scanners: A Comparative Analysis    | [https://arxiv.org/abs/2410.16527](https://arxiv.org/abs/2410.16527)                                                                                                                                                                 |                                                                                                                                                                | [
