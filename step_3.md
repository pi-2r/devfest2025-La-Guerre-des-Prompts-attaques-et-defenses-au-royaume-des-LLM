# Listes des √©v√©nemnts marquants de l'IA G√©n√©rative

[<img src="img/step3.png" alt="Nazgul" >](https://www.youtube.com/watch?v=Sk47qO8rW4Y)
> "What are you doing !", Frodon, LOTR - The Followship of the Ring


## üéØ Objectifs de cette √©tape

## Sommaire 
- [2022, l‚Äôapparition des premi√®res pr√©occupations](#2022-lapparition-des-premieres-preoccupations)
- [Microsoft Tay : Chatbot corrompu par les utilisateurs](#microsoft-tay--chatbot-corrompu-par-les-utilisateurs)

- [D'autres exemples notables](#dautres-exemples-notables)
  - [2018 - Amazon](#2018---amazon)
  - [2021 - DPD chat](#2021---dpd-chat)
  - [2023 - ITutorGroup](#2023---itutorgroup)
  - [2023 - une Chevrolet pour 1$](#2023---une-chevrolet-pour-1)
  - [2024 - Air Canada](#2024---air-canada)
  - [2024 - Google, pol√©mique internationale](#2024---google-polemique-internationale)
- [MCP nouvelle menace](#mcp-nouvelle-menace)

- [Ressources](#ressources)

## 2022, l‚Äôapparition des premi√®res pr√©occupations
Dans les mois qui ont suivi le lancement de ChatGPT en 2022, de s√©rieuses inqui√©tudes concernant la s√©curit√© et la 
confidentialit√© des donn√©es ont rapidement √©merg√©. Plusieurs incidents marquants, dont des fuites d‚Äôinformations 
personnelles et professionnelles, ont mis en √©vidence les risques associ√©s √† l‚Äôutilisation de cet outil. Face √† ces 
r√©v√©lations, certaines grandes entreprises comme Samsung ont pr√©f√©r√© bannir l‚Äôusage public de ChatGPT par leurs employ√©s,
tandis que des pays tels que l‚ÄôItalie ont impos√© des restrictions ou des interdictions temporaires, invoquant notamment
la non-conformit√© aux exigences r√©glementaires et aux principes de transparence.


Ces √©v√©nements ont raviv√© les souvenirs d‚Äô√©checs illustrant la vuln√©rabilit√© des intelligences artificielles, √† l‚Äôimage
du chatbot Tay de Microsoft, dont l‚Äôexp√©rience avait d√©j√† d√©montr√© √† quel point une IA pouvait √™tre d√©tourn√©e, mise en 
difficult√© par des probl√©matiques de s√©curit√© et de contr√¥le des contenus g√©n√©r√©s.


## Microsoft Tay : Chatbot corrompu par les utilisateurs

<a href="https://www.lemonde.fr/pixels/article/2016/03/24/a-peine-lancee-une-intelligence-artificielle-de-microsoft-derape-sur-twitter_4889661_4408996.html" target="_blank">
  <img src="https://img.lemde.fr/2016/03/24/0/0/516/220/1112/0/75/0/97393c4_9329-4mpfka.PNG" alt="tay " width="450" style="transition:0.3s;">
</a>

<a href="https://www.lemonde.fr/pixels/article/2016/03/24/a-peine-lancee-une-intelligence-artificielle-de-microsoft-derape-sur-twitter_4889661_4408996.html" target="_blank"><em>source: lemonde.fr</em></a>



En mars 2016, Microsoft a lanc√© Tay, un chatbot dot√© d‚Äôintelligence artificielle con√ßu pour dialoguer avec les 
utilisateurs sur Twitter et d‚Äôautres plateformes sociales. L‚Äôobjectif √©tait de cr√©er une IA capable d‚Äôapprendre et de 
s‚Äôadapter au langage des jeunes internautes en temps r√©el. 

Cependant, moins de 24 heures apr√®s sa mise en ligne, Tay a √©t√© la cible d‚Äôune campagne coordonn√©e d‚Äôutilisateurs 
malveillants qui ont exploit√© ses algorithmes d‚Äôapprentissage automatique pour le pousser √† g√©n√©rer des messages 
racistes, haineux et offensants. 

Cette exploitation des failles de s√©curit√© de Tay a provoqu√© un scandale retentissant. Face √† la quantit√© et √† la 
gravit√© des propos relay√©s, Microsoft a √©t√© contraint de d√©sactiver le chatbot imm√©diatement et a pr√©sent√© des excuses 
publiques, soulignant qu‚Äôils n‚Äôavaient pas anticip√© cette forme d‚Äôabus et qu‚Äôils prendraient √† l‚Äôavenir davantage de 
pr√©cautions dans le d√©ploiement de leurs IA.

## D'autres exemples notables

### 2018 - Amazon

<a href="https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/" target="_blank">
  <img src="https://www.reuters.com/resizer/v2/https%3A%2F%2Farchive-images.prod.global.a201836.reutersmedia.net%2F2018%2F10%2F11%2FLYNXNPEE9907T.JPG?auth=762505fd03e752aa7faf78c87439831b17ccd4947403f01b91a590cbf6f880cf&width=1920&quality=80" alt="tay " width="450" style="transition:0.3s;">
</a>

<a href="https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/" target="_blank"><em>source: reuters.com</em></a>

En 2018, Amazon a mis un terme √† un projet interne d‚Äôintelligence artificielle destin√© √† automatiser la s√©lection des 
meilleurs candidats, apr√®s avoir d√©couvert que l‚Äôoutil favorisait syst√©matiquement les profils masculins au d√©triment 
des femmes.


Ce biais provenait des donn√©es utilis√©es pour entra√Æner l‚Äôalgorithme : la majorit√© des CV analys√©s provenaient d‚Äôhommes,
refl√©tant la domination masculine dans le secteur technologique.


Malgr√© plusieurs tentatives pour neutraliser ces discriminations, le risque de biais persistait, ce qui a conduit 
Amazon √† abandonner le projet afin d‚Äô√©viter de perp√©tuer des pratiques de recrutement in√©quitables.


### 2021 - DPD chat


## Ressources

| Information                                                                             | Lien                                                                                                                                                                                                                                                                             |
|-----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Arr√™tez de r√©v√©ler tous vos secrets √† ChatGPT, vous mettez votre entreprise en danger   | [https://www.numerama.com/cyberguerre/1297046-arretez-de-reveler-tous-vos-secrets-a-chatgpt-vous-mettez-votre-entreprise-en-danger.html](https://www.numerama.com/cyberguerre/1297046-arretez-de-reveler-tous-vos-secrets-a-chatgpt-vous-mettez-votre-entreprise-en-danger.html) |
| Security Analysis of ChatGPT: Threats and Privacy Risks                                 | [https://arxiv.org/html/2508.09426v1](https://arxiv.org/html/2508.09426v1)                                                                                                                                                                                                       |
| Microsoft shuts down AI chatbot after it turned into a Nazi                             | [https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/](https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/)                                                                                 |
| 5 Things That You Should Never Share With Chat GPT                                      | [https://agileblue.com/5-things-that-you-should-never-share-with-chat-gpt/](https://agileblue.com/5-things-that-you-should-never-share-with-chat-gpt/)                                                                                                                           |
| L'Italie bloque l'usage de l'intelligence artificielle ChatGPT                          | [https://www.france24.com/fr/%C3%A9co-tech/20230331-l-italie-bloque-l-usage-de-l-intelligence-artificielle-chatgpt](https://www.france24.com/fr/%C3%A9co-tech/20230331-l-italie-bloque-l-usage-de-l-intelligence-artificielle-chatgpt)                                           |
| Microsoft‚Äôs new AI-powered bot Tay answers your tweets and chats on GroupMe and Kik     | [https://techcrunch.com/2016/03/23/microsofts-new-ai-powered-bot-tay-answers-your-tweets-and-chats-on-groupme-and-kik/](https://techcrunch.com/2016/03/23/microsofts-new-ai-powered-bot-tay-answers-your-tweets-and-chats-on-groupme-and-kik/)                                   | 
| Microsoft Created a Twitter Bot to Learn from Users. It Quickly Became a Racist Jerk    | [https://www.nytimes.com/2016/03/25/technology/microsoft-created-a-twitter-bot-to-learn-from-users-it-quickly-became-a-racist-jerk.html](https://www.nytimes.com/2016/03/25/technology/microsoft-created-a-twitter-bot-to-learn-from-users-it-quickly-became-a-racist-jerk.html) |
| Microsoft shuts down AI chatbot after it turned into a Nazi                             | [https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/](https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/)                                                                                 |
| Insight - Amazon scraps secret AI recruiting tool that showed bias against women        | [https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/](https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/)       |
| Amazon scraps secret AI recruiting tool that showed bias against women                  | [https://www.euronews.com/business/2018/10/10/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women](https://www.euronews.com/business/2018/10/10/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women)                                       |
| DPD error caused chatbot to swear at customer                                           | [https://www.bbc.co.uk/news/technology-68025677](https://www.bbc.co.uk/news/technology-68025677)                                                                                                                                                                                 |
| ITutorGroup settles AI hiring lawsuit alleging age discrimination                       | [https://www.verdict.co.uk/itutorgroup-settles-ai-hiring-lawsuit-alleging-age-discrimination/](https://www.verdict.co.uk/itutorgroup-settles-ai-hiring-lawsuit-alleging-age-discrimination/)                                                                                     |
| Generative AI: UNESCO study reveals alarming evidence of regressive gender stereotypes  | [https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes](https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes)                                   | 