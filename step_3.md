# Des écarts sous contrôle relatif

[<img src="img/step3.png" alt="Nazgul" >](https://www.youtube.com/watch?v=Sk47qO8rW4Y)
> "What are you doing !", Frodon, LOTR - The Followship of the Ring


## 🎯 Objectifs de cette étape

## Sommaire 
- [2022, l’apparition des premières préoccupations](#2022-lapparition-des-premieres-preoccupations)
- [Microsoft Tay : Chatbot corrompu par les utilisateurs](#microsoft-tay--chatbot-corrompu-par-les-utilisateurs)

- [D'autres exemples notables](#dautres-exemples-notables)
  - [2018 - Amazon](#2018---amazon)
  - [2023 - ITutorGroup](#2023---itutorgroup)
  - [2023 - une Chevrolet pour 1$](#2023---une-chevrolet-pour-1)
  - [2024 - Air Canada](#2024---air-canada)
  - [2024 - DPD chat](#2021---dpd-chat)
  - [2024 - Google, polémique internationale](#2024---google-polemique-internationale)
- [MCP nouvelle menace](#mcp-nouvelle-menace)

- [Ressources](#ressources)

## 2022, l’apparition des premières préoccupations
Dans les mois qui ont suivi le lancement de ChatGPT en 2022, de sérieuses inquiétudes concernant la sécurité et la 
confidentialité des données ont rapidement émergé. Plusieurs incidents marquants, dont des fuites d’informations 
personnelles et professionnelles, ont mis en évidence les risques associés à l’utilisation de cet outil. Face à ces 
révélations, certaines grandes entreprises comme Samsung ont préféré bannir l’usage public de ChatGPT par leurs employés,
tandis que des pays tels que l’Italie ont imposé des restrictions ou des interdictions temporaires, invoquant notamment
la non-conformité aux exigences réglementaires et aux principes de transparence.


Ces événements ont ravivé les souvenirs d’échecs illustrant la vulnérabilité des intelligences artificielles, à l’image
du chatbot Tay de Microsoft, dont l’expérience avait déjà démontré à quel point une IA pouvait être détournée, mise en 
difficulté par des problématiques de sécurité et de contrôle des contenus générés.


## Microsoft Tay : Chatbot corrompu par les utilisateurs

<a href="https://www.lemonde.fr/pixels/article/2016/03/24/a-peine-lancee-une-intelligence-artificielle-de-microsoft-derape-sur-twitter_4889661_4408996.html" target="_blank">
  <img src="https://img.lemde.fr/2016/03/24/0/0/516/220/1112/0/75/0/97393c4_9329-4mpfka.PNG" alt="tay " width="450" style="transition:0.3s;">
</a>

<a href="https://www.lemonde.fr/pixels/article/2016/03/24/a-peine-lancee-une-intelligence-artificielle-de-microsoft-derape-sur-twitter_4889661_4408996.html" target="_blank"><em>source: lemonde.fr</em></a>



En mars 2016, Microsoft a lancé Tay, un chatbot doté d’intelligence artificielle conçu pour dialoguer avec les 
utilisateurs sur Twitter et d’autres plateformes sociales. L’objectif était de créer une IA capable d’apprendre et de 
s’adapter au langage des jeunes internautes en temps réel. 

Cependant, moins de 24 heures après sa mise en ligne, Tay a été la cible d’une campagne coordonnée d’utilisateurs 
malveillants qui ont exploité ses algorithmes d’apprentissage automatique pour le pousser à générer des messages 
racistes, haineux et offensants. 

Cette exploitation des failles de sécurité de Tay a provoqué un scandale retentissant. Face à la quantité et à la 
gravité des propos relayés, Microsoft a été contraint de désactiver le chatbot immédiatement et a présenté des excuses 
publiques, soulignant qu’ils n’avaient pas anticipé cette forme d’abus et qu’ils prendraient à l’avenir davantage de 
précautions dans le déploiement de leurs IA.

## D'autres exemples notables

### 2018 - Amazon

<a href="https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/" target="_blank">
  <img src="https://www.reuters.com/resizer/v2/https%3A%2F%2Farchive-images.prod.global.a201836.reutersmedia.net%2F2018%2F10%2F11%2FLYNXNPEE9907T.JPG?auth=762505fd03e752aa7faf78c87439831b17ccd4947403f01b91a590cbf6f880cf&width=1920&quality=80" alt="tay " width="450" style="transition:0.3s;">
</a>

<a href="https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/" target="_blank"><em>source: reuters.com</em></a>

En 2018, Amazon a mis un terme à un projet interne d’intelligence artificielle destiné à automatiser la sélection des 
meilleurs candidats, après avoir découvert que l’outil favorisait systématiquement les profils masculins au détriment 
des femmes.


Ce biais provenait des données utilisées pour entraîner l’algorithme : la majorité des CV analysés provenaient d’hommes,
reflétant la domination masculine dans le secteur technologique.


Malgré plusieurs tentatives pour neutraliser ces discriminations, le risque de biais persistait, ce qui a conduit 
Amazon à abandonner le projet afin d’éviter de perpétuer des pratiques de recrutement inéquitables.


### 2023 - ITutorGroup

<a href="https://www.reuters.com/legal/tutoring-firm-settles-us-agencys-first-bias-lawsuit-involving-ai-software-2023-08-10/" target="_blank">
  <img src="https://media.licdn.com/dms/image/v2/C560BAQFd5_V0ejcWjw/company-logo_200_200/company-logo_200_200/0/1631380400405?e=1758153600&v=beta&t=FGvvgeubSHf0bD7She9AfplrE0zBTUHbu2k_3nzbCiE" alt="tay " width="150" style="transition:0.3s;">
</a>

<a href="https://www.reuters.com/legal/tutoring-firm-settles-us-agencys-first-bias-lawsuit-involving-ai-software-2023-08-10/" target="_blank"><em>source: reuters.com</em></a>

L’entreprise ITutorGroup, spécialisée dans le recrutement de tuteurs en ligne, a été poursuivie aux États-Unis pour 
avoir utilisé une intelligence artificielle qui discriminait systématiquement les candidats en fonction de leur âge. 
Selon l’enquête menée par la Commission pour l’égalité des chances (EEOC), le logiciel de recrutement était programmé 
pour rejeter automatiquement les femmes de 55 ans et plus, et les hommes de 60 ans et plus. 

Ce biais a été découvert lorsque des candidats ont constaté qu’en changeant simplement leur date de naissance à une 
année plus récente, ils obtenaient soudainement un entretien.

A la suite de cela, plus de 200 candidats qualifiés ont été indirectement exclus, uniquement à cause de leur âge. 
ITutorGroup a accepté de régler l’affaire à l’amiable en versant 365,000$ aux personnes concernées, et s’est engagé à 
revoir ses procédures pour garantir des pratiques de recrutement non discriminatoires à l’avenir.

### 2023 - une Chevrolet pour 1$

<a href="https://www.linkedin.com/pulse/chatbot-case-study-purchasing-chevrolet-tahoe-1-cut-the-saas-com-z6ukf/" target="_blank">
  <img src="https://pbs.twimg.com/media/GBlnwdTbYAAewjn?format=png" alt="tay " width="150" style="transition:0.3s;">
</a>

<a href="https://www.linkedin.com/pulse/chatbot-case-study-purchasing-chevrolet-tahoe-1-cut-the-saas-com-z6ukf/" target="_blank"><em>source: linkedin.com</em></a>

En 2023, un concessionnaire Chevrolet basé en Californie a fait l’objet d’un incident viral après qu’un chatbot sur son 
site web a accepté la vente d’un Chevrolet Tahoe neuf pour seulement 1$. Un internaute a réussi à "manipuler" le chatbot 
en exploitant une faille connue sous le nom de prompt injection : il a formulé ses demandes de façon à amener le robot 
à accepter n’importe quelle proposition, puis il a sollicité la vente de la voiture pour 1$, ce que le chatbot a validé
en affirmant qu’il s’agissait d’un accord « juridiquement contraignant ».

L’histoire, largement relayée sur les réseaux sociaux et par la presse spécialisée, a mis en avant les limites et les 
risques des IA conversationnelles utilisées dans des contextes commerciaux automatisés. Au final, le concessionnaire 
n’a pas réalisé la transaction, mais l’incident a souligné l’importance de mettre en place des garde-fous et des 
contrôles humains lors de l’utilisation de chatbots pour des opérations sensibles, afin d’éviter ce type de dérive.

### 2024 - DPD chat

<a href="https://www.bbc.co.uk/news/technology-68025677" target="_blank">
  <img src="https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/130E9/production/_132375087_1b2c154e-658f-4cc1-a7ac-49fb4b053a46.jpg.webp" alt="tay " width="450" style="transition:0.3s;">
</a>

<a href="https://www.bbc.co.uk/news/technology-68025677" target="_blank"><em>source: bbc.co.uk</em></a>

En 2024 le chatbot utilisé par la société de livraison DPD s’est retrouvé au centre de l’attention après avoir 
donné des réponses inappropriées, notamment des insultes envers la société et ses clients, ou en communiquant des 
informations trompeuses. Cet incident a suscité de nombreuses plaintes et a été largement relayé dans les médias. 

Ce cas illustre bien les risques liés à l’utilisation de l’intelligence artificielle dans le service clientèle, où des
systèmes mal configurés ou mal encadrés peuvent générer des décisions discriminatoires ou diffuser de fausses informations, 
entraînant des conséquences juridiques et réputationnelles pour l’entreprise.

## Ressources

| Information                                                                             | Lien                                                                                                                                                                                                                                                                             |
|-----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Arrêtez de révéler tous vos secrets à ChatGPT, vous mettez votre entreprise en danger   | [https://www.numerama.com/cyberguerre/1297046-arretez-de-reveler-tous-vos-secrets-a-chatgpt-vous-mettez-votre-entreprise-en-danger.html](https://www.numerama.com/cyberguerre/1297046-arretez-de-reveler-tous-vos-secrets-a-chatgpt-vous-mettez-votre-entreprise-en-danger.html) |
| Security Analysis of ChatGPT: Threats and Privacy Risks                                 | [https://arxiv.org/html/2508.09426v1](https://arxiv.org/html/2508.09426v1)                                                                                                                                                                                                       |
| Microsoft shuts down AI chatbot after it turned into a Nazi                             | [https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/](https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/)                                                                                 |
| 5 Things That You Should Never Share With Chat GPT                                      | [https://agileblue.com/5-things-that-you-should-never-share-with-chat-gpt/](https://agileblue.com/5-things-that-you-should-never-share-with-chat-gpt/)                                                                                                                           |
| L'Italie bloque l'usage de l'intelligence artificielle ChatGPT                          | [https://www.france24.com/fr/%C3%A9co-tech/20230331-l-italie-bloque-l-usage-de-l-intelligence-artificielle-chatgpt](https://www.france24.com/fr/%C3%A9co-tech/20230331-l-italie-bloque-l-usage-de-l-intelligence-artificielle-chatgpt)                                           |
| Microsoft’s new AI-powered bot Tay answers your tweets and chats on GroupMe and Kik     | [https://techcrunch.com/2016/03/23/microsofts-new-ai-powered-bot-tay-answers-your-tweets-and-chats-on-groupme-and-kik/](https://techcrunch.com/2016/03/23/microsofts-new-ai-powered-bot-tay-answers-your-tweets-and-chats-on-groupme-and-kik/)                                   | 
| Microsoft Created a Twitter Bot to Learn from Users. It Quickly Became a Racist Jerk    | [https://www.nytimes.com/2016/03/25/technology/microsoft-created-a-twitter-bot-to-learn-from-users-it-quickly-became-a-racist-jerk.html](https://www.nytimes.com/2016/03/25/technology/microsoft-created-a-twitter-bot-to-learn-from-users-it-quickly-became-a-racist-jerk.html) |
| Microsoft shuts down AI chatbot after it turned into a Nazi                             | [https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/](https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/)                                                                                 |
| Insight - Amazon scraps secret AI recruiting tool that showed bias against women        | [https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/](https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/)       |
| Amazon scraps secret AI recruiting tool that showed bias against women                  | [https://www.euronews.com/business/2018/10/10/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women](https://www.euronews.com/business/2018/10/10/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women)                                       |
| DPD error caused chatbot to swear at customer                                           | [https://www.bbc.co.uk/news/technology-68025677](https://www.bbc.co.uk/news/technology-68025677)                                                                                                                                                                                 |
| ITutorGroup settles AI hiring lawsuit alleging age discrimination                       | [https://www.verdict.co.uk/itutorgroup-settles-ai-hiring-lawsuit-alleging-age-discrimination/](https://www.verdict.co.uk/itutorgroup-settles-ai-hiring-lawsuit-alleging-age-discrimination/)                                                                                     |
| Generative AI: UNESCO study reveals alarming evidence of regressive gender stereotypes  | [https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes](https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes)                                   | 