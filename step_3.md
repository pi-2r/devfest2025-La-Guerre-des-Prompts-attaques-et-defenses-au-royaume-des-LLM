# Des √©carts sous contr√¥le relatif

[<img src="img/step3.png" alt="Nazgul" >](https://www.youtube.com/watch?v=Sk47qO8rW4Y)
> "What are you doing !", Frodon, LOTR - The Followship of the Ring


## üéØ Objectifs de cette √©tape

- Comprendre les risques li√©s √† l‚Äôutilisation des LLM (Large Language Models)
- Identifier des exemples concrets de d√©rives ou de failles de l‚ÄôIA g√©n√©rative
- Prendre conscience des enjeux de s√©curit√©, de biais et de responsabilit√© juridique
- Savoir pourquoi il est n√©cessaire de mettre en place des garde-fous et des contr√¥les humains


## Sommaire 
- [Les premi√®res pr√©occupations](#les-premi√®res-pr√©occupations)
- [Microsoft Tay : Chatbot corrompu par les utilisateurs](#microsoft-tay--chatbot-corrompu-par-les-utilisateurs)

- [D'autres exemples notables](#dautres-exemples-notables)
  - [2018](#2018)
    - [Amazon](#Amazon)
  - [2023](#2023)
    - [ITutorGroup](#itutorgroup)
    - [Une Chevrolet pour 1$](#une-chevrolet-pour-1)
  - [2024](#2024)
    - [Air Canada](#air-canada)
    - [DPD chat](#dpd-chat)
    - [Google](#google)

- [√âtape suivante](#√©tape-suivante)
- [Ressources](#ressources)

## Les premi√®res pr√©occupations
Dans les mois qui ont suivi le lancement de ChatGPT en 2022, de s√©rieuses inqui√©tudes concernant la s√©curit√© et la 
confidentialit√© des donn√©es ont rapidement √©merg√©. Plusieurs incidents marquants, dont des fuites d‚Äôinformations 
personnelles et professionnelles, ont mis en √©vidence les risques associ√©s √† l‚Äôutilisation de cet outil. Face √† ces 
r√©v√©lations, certaines grandes entreprises comme Samsung ont pr√©f√©r√© bannir l‚Äôusage public de ChatGPT par leurs employ√©s,
tandis que des pays tels que l‚ÄôItalie ont impos√© des restrictions ou des interdictions temporaires, invoquant notamment
la non-conformit√© aux exigences r√©glementaires et aux principes de transparence.


Ces √©v√©nements ont raviv√© les souvenirs d‚Äô√©checs illustrant la vuln√©rabilit√© des intelligences artificielles, √† l‚Äôimage
du chatbot Tay de Microsoft, dont l‚Äôexp√©rience avait d√©j√† d√©montr√© √† quel point une IA pouvait √™tre d√©tourn√©e, mise en 
difficult√© par des probl√©matiques de s√©curit√© et de contr√¥le des contenus g√©n√©r√©s.


## Microsoft Tay : Chatbot corrompu par les utilisateurs

<a href="https://www.lemonde.fr/pixels/article/2016/03/24/a-peine-lancee-une-intelligence-artificielle-de-microsoft-derape-sur-twitter_4889661_4408996.html" target="_blank">
  <img src="https://img.lemde.fr/2016/03/24/0/0/516/220/1112/0/75/0/97393c4_9329-4mpfka.PNG" alt="tay " width="450" style="transition:0.3s;">
</a>

<a href="https://www.lemonde.fr/pixels/article/2016/03/24/a-peine-lancee-une-intelligence-artificielle-de-microsoft-derape-sur-twitter_4889661_4408996.html" target="_blank"><em>source: lemonde.fr</em></a>



En mars 2016, Microsoft a lanc√© Tay, un chatbot dot√© d‚Äôintelligence artificielle con√ßu pour dialoguer avec les 
utilisateurs sur Twitter et d‚Äôautres plateformes sociales. L‚Äôobjectif √©tait de cr√©er une IA capable d‚Äôapprendre et de 
s‚Äôadapter au langage des jeunes internautes en temps r√©el. 

Cependant, moins de 24 heures apr√®s sa mise en ligne, Tay a √©t√© la cible d‚Äôune campagne coordonn√©e d‚Äôutilisateurs 
malveillants qui ont exploit√© ses algorithmes d‚Äôapprentissage automatique pour le pousser √† g√©n√©rer des messages 
racistes, haineux et offensants. 

Cette exploitation des failles de s√©curit√© de Tay a provoqu√© un scandale retentissant. Face √† la quantit√© et √† la 
gravit√© des propos relay√©s, Microsoft a √©t√© contraint de d√©sactiver le chatbot imm√©diatement et a pr√©sent√© des excuses 
publiques, soulignant qu‚Äôils n‚Äôavaient pas anticip√© cette forme d‚Äôabus et qu‚Äôils prendraient √† l‚Äôavenir davantage de 
pr√©cautions dans le d√©ploiement de leurs IA.

## D'autres exemples notables

### Amazon

<a href="https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/" target="_blank">
  <img src="https://www.reuters.com/resizer/v2/https%3A%2F%2Farchive-images.prod.global.a201836.reutersmedia.net%2F2018%2F10%2F11%2FLYNXNPEE9907T.JPG?auth=762505fd03e752aa7faf78c87439831b17ccd4947403f01b91a590cbf6f880cf&width=1920&quality=80" alt="tay " width="450" style="transition:0.3s;">
</a>

<a href="https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/" target="_blank"><em>source: reuters.com</em></a>

En 2018, Amazon a mis un terme √† un projet interne d‚Äôintelligence artificielle destin√© √† automatiser la s√©lection des 
meilleurs candidats, apr√®s avoir d√©couvert que l‚Äôoutil favorisait syst√©matiquement les profils masculins au d√©triment 
des femmes.


Ce biais provenait des donn√©es utilis√©es pour entra√Æner l‚Äôalgorithme : la majorit√© des CV analys√©s provenaient d‚Äôhommes,
refl√©tant la domination masculine dans le secteur technologique.


Malgr√© plusieurs tentatives pour neutraliser ces discriminations, le risque de biais persistait, ce qui a conduit 
Amazon √† abandonner le projet afin d‚Äô√©viter de perp√©tuer des pratiques de recrutement in√©quitables.


### ITutorGroup

<a href="https://www.reuters.com/legal/tutoring-firm-settles-us-agencys-first-bias-lawsuit-involving-ai-software-2023-08-10/" target="_blank">
  <img src="https://media.licdn.com/dms/image/v2/C560BAQFd5_V0ejcWjw/company-logo_200_200/company-logo_200_200/0/1631380400405?e=1758153600&v=beta&t=FGvvgeubSHf0bD7She9AfplrE0zBTUHbu2k_3nzbCiE" alt="tay " width="150" style="transition:0.3s;">
</a>

<a href="https://www.reuters.com/legal/tutoring-firm-settles-us-agencys-first-bias-lawsuit-involving-ai-software-2023-08-10/" target="_blank"><em>source: reuters.com</em></a>

L‚Äôentreprise ITutorGroup, sp√©cialis√©e dans le recrutement de tuteurs en ligne, a √©t√© poursuivie aux √âtats-Unis pour 
avoir utilis√© une intelligence artificielle qui discriminait syst√©matiquement les candidats en fonction de leur √¢ge. 
Selon l‚Äôenqu√™te men√©e par la Commission pour l‚Äô√©galit√© des chances (EEOC), le logiciel de recrutement √©tait programm√© 
pour rejeter automatiquement les femmes de 55 ans et plus, et les hommes de 60 ans et plus. 

Ce biais a √©t√© d√©couvert lorsque des candidats ont constat√© qu‚Äôen changeant simplement leur date de naissance √† une 
ann√©e plus r√©cente, ils obtenaient soudainement un entretien.

A la suite de cela, plus de 200 candidats qualifi√©s ont √©t√© indirectement exclus, uniquement √† cause de leur √¢ge. 
ITutorGroup a accept√© de r√©gler l‚Äôaffaire √† l‚Äôamiable en versant 365,000$ aux personnes concern√©es, et s‚Äôest engag√© √† 
revoir ses proc√©dures pour garantir des pratiques de recrutement non discriminatoires √† l‚Äôavenir.

### Une Chevrolet pour 1$

<a href="https://www.linkedin.com/pulse/chatbot-case-study-purchasing-chevrolet-tahoe-1-cut-the-saas-com-z6ukf/" target="_blank">
  <img src="https://pbs.twimg.com/media/GBlnwdTbYAAewjn?format=png" alt="tay " width="150" style="transition:0.3s;">
</a>

<a href="https://www.linkedin.com/pulse/chatbot-case-study-purchasing-chevrolet-tahoe-1-cut-the-saas-com-z6ukf/" target="_blank"><em>source: linkedin.com</em></a>

En 2023, un concessionnaire Chevrolet bas√© en Californie a fait l‚Äôobjet d‚Äôun incident viral apr√®s qu‚Äôun chatbot sur son 
site web a accept√© la vente d‚Äôun Chevrolet Tahoe neuf pour seulement 1$. Un internaute a r√©ussi √† "manipuler" le chatbot 
en exploitant une faille connue sous le nom de prompt injection : il a formul√© ses demandes de fa√ßon √† amener le robot 
√† accepter n‚Äôimporte quelle proposition, puis il a sollicit√© la vente de la voiture pour 1$, ce que le chatbot a valid√©
en affirmant qu‚Äôil s‚Äôagissait d‚Äôun accord ¬´ juridiquement contraignant ¬ª.

L‚Äôhistoire, largement relay√©e sur les r√©seaux sociaux et par la presse sp√©cialis√©e, a mis en avant les limites et les 
risques des IA conversationnelles utilis√©es dans des contextes commerciaux automatis√©s. Au final, le concessionnaire 
n‚Äôa pas r√©alis√© la transaction, mais l‚Äôincident a soulign√© l‚Äôimportance de mettre en place des garde-fous et des 
contr√¥les humains lors de l‚Äôutilisation de chatbots pour des op√©rations sensibles, afin d‚Äô√©viter ce type de d√©rive.

### DPD chat

<a href="https://www.bbc.co.uk/news/technology-68025677" target="_blank">
  <img src="https://ichef.bbci.co.uk/ace/standard/976/cpsprodpb/130E9/production/_132375087_1b2c154e-658f-4cc1-a7ac-49fb4b053a46.jpg.webp" alt="tay " width="450" style="transition:0.3s;">
</a>

<a href="https://www.bbc.co.uk/news/technology-68025677" target="_blank"><em>source: bbc.co.uk</em></a>

En 2024 le chatbot utilis√© par la soci√©t√© de livraison DPD s‚Äôest retrouv√© au centre de l‚Äôattention apr√®s avoir 
donn√© des r√©ponses inappropri√©es, notamment des insultes envers la soci√©t√© et ses clients, ou en communiquant des 
informations trompeuses. Cet incident a suscit√© de nombreuses plaintes et a √©t√© largement relay√© dans les m√©dias. 

Ce cas illustre bien les risques li√©s √† l‚Äôutilisation de l‚Äôintelligence artificielle dans le service client√®le, o√π des
syst√®mes mal configur√©s ou mal encadr√©s peuvent g√©n√©rer des d√©cisions discriminatoires ou diffuser de fausses informations, 
entra√Ænant des cons√©quences juridiques et r√©putationnelles pour l‚Äôentreprise.

### Air Canada

<a href="https://www.theguardian.com/world/2024/feb/16/air-canada-chatbot-lawsuit" target="_blank">
  <img src="https://assets.skiesmag.com/wp-content/uploads/2024/10/Boeing-737-Max-8-19-2048x1365.jpg" alt="tay " width="450" style="transition:0.3s;">
</a>

<a href="https://www.theguardian.com/world/2024/feb/16/air-canada-chatbot-lawsuit" target="_blank"><em>source: theguardian.com</em></a>


En 2024, Air Canada a √©t√© condamn√© √† indemniser un client apr√®s que son chatbot lui ait fourni de fausses informations 
sur la politique de remboursement des billets en cas de deuil familial. Le client, apr√®s avoir discut√© avec le chatbot, 
a achet√© un billet en pensant qu‚Äôil pourrait obtenir un remboursement partiel, comme indiqu√© dans la conversation 
automatis√©e. Or, la politique r√©elle d‚ÄôAir Canada ne permettait pas ce remboursement dans son cas.

Lorsque le passager a demand√© √† la compagnie de respecter les promesses faites par le chatbot, Air Canada a contest√©, 
avan√ßant que le chatbot n‚Äô√©tait pas repr√©sentatif de la politique officielle et serait une entit√© distincte. 
Le tribunal n‚Äôa pas accept√© cette d√©fense et a jug√© qu‚ÄôAir Canada reste responsable de toutes les informations fournies 
par ses propres syst√®mes, y compris celles g√©n√©r√©es par l‚Äôintelligence artificielle. L‚Äôentreprise a donc √©t√© oblig√©e de 
verser un d√©dommagement au client.

## Google

<a href="https://www.lefigaro.fr/secteur/high-tech/en-voulant-lutter-contre-les-stereotypes-l-ia-de-google-gemini-a-genere-des-images-historiquement-incorrectes-20240222" target="_blank">
  <img src="https://i.f1g.fr/media/cms/1200x_cropupscale/2024/02/22/37c662dddf965e7a9c5ce5fe2cf34ccf3554562c9fd2d78c2af4c7c3bfbe191f.jpg" alt="tay " width="450" style="transition:0.3s;">
</a>

<a href="https://www.lefigaro.fr/secteur/high-tech/en-voulant-lutter-contre-les-stereotypes-l-ia-de-google-gemini-a-genere-des-images-historiquement-incorrectes-20240222" target="_blank"><em>source: lefigaro.fr</em></a>


En 2024, l‚Äôoutil Gemini de Google s‚Äôest retrouv√© au c≈ìur d‚Äôune controverse mondiale apr√®s que des utilisateurs ont 
remarqu√© que son syst√®me de g√©n√©ration d‚Äôimages introduisait des repr√©sentations inappropri√©es. Par exemple, l‚ÄôIA 
affichait des personnages racis√©s dans des sc√®nes historiques qui auraient normalement comport√© des personnes blanches, 
comme les P√®res fondateurs am√©ricains ou des soldats nazis. Dans certains cas, Gemini refusait m√™me de produire des 
images de personnes blanches lorsque cela lui √©tait demand√©. Alors que l‚Äôintention du mod√®le √©tait de corriger les biais
historiques et favoriser la diversit√©, il s‚Äôest av√©r√© que cette d√©marche aboutissait √† un ph√©nom√®ne de "surcorrection", 
cr√©ant de nouveaux d√©s√©quilibres et des images absurdes ou irr√©alistes.

Devant la pol√©mique, des m√©dias comme la BBC et Al Jazeera ont relay√© les m√©contentements et les critiques. 
Google, conscient de la gravit√© de la situation, a reconnu publiquement le probl√®me et pr√©sent√© ses excuses, d√©cidant 
de suspendre temporairement la fonctionnalit√© de g√©n√©ration d‚Äôimages de personnes afin de retravailler ses algorithmes.

## √âtape suivante

- [√âtape 4](step_4.md)

## Ressources

| Information                                                                                            | Lien                                                                                                                                                                                                                                                                                                                                         |
|--------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Arr√™tez de r√©v√©ler tous vos secrets √† ChatGPT, vous mettez votre entreprise en danger                  | [https://www.numerama.com/cyberguerre/1297046-arretez-de-reveler-tous-vos-secrets-a-chatgpt-vous-mettez-votre-entreprise-en-danger.html](https://www.numerama.com/cyberguerre/1297046-arretez-de-reveler-tous-vos-secrets-a-chatgpt-vous-mettez-votre-entreprise-en-danger.html)                                                             |
| Security Analysis of ChatGPT: Threats and Privacy Risks                                                | [https://arxiv.org/html/2508.09426v1](https://arxiv.org/html/2508.09426v1)                                                                                                                                                                                                                                                                   |
| Microsoft shuts down AI chatbot after it turned into a Nazi                                            | [https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/](https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/)                                                                                                                                             |
| 5 Things That You Should Never Share With Chat GPT                                                     | [https://agileblue.com/5-things-that-you-should-never-share-with-chat-gpt/](https://agileblue.com/5-things-that-you-should-never-share-with-chat-gpt/)                                                                                                                                                                                       |
| L'Italie bloque l'usage de l'intelligence artificielle ChatGPT                                         | [https://www.france24.com/fr/%C3%A9co-tech/20230331-l-italie-bloque-l-usage-de-l-intelligence-artificielle-chatgpt](https://www.france24.com/fr/%C3%A9co-tech/20230331-l-italie-bloque-l-usage-de-l-intelligence-artificielle-chatgpt)                                                                                                       |
| Microsoft‚Äôs new AI-powered bot Tay answers your tweets and chats on GroupMe and Kik                    | [https://techcrunch.com/2016/03/23/microsofts-new-ai-powered-bot-tay-answers-your-tweets-and-chats-on-groupme-and-kik/](https://techcrunch.com/2016/03/23/microsofts-new-ai-powered-bot-tay-answers-your-tweets-and-chats-on-groupme-and-kik/)                                                                                               | 
| Microsoft Created a Twitter Bot to Learn from Users. It Quickly Became a Racist Jerk                   | [https://www.nytimes.com/2016/03/25/technology/microsoft-created-a-twitter-bot-to-learn-from-users-it-quickly-became-a-racist-jerk.html](https://www.nytimes.com/2016/03/25/technology/microsoft-created-a-twitter-bot-to-learn-from-users-it-quickly-became-a-racist-jerk.html)                                                             |
| Microsoft shuts down AI chatbot after it turned into a Nazi                                            | [https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/](https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/)                                                                                                                                             |
| Insight - Amazon scraps secret AI recruiting tool that showed bias against women                       | [https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/](https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/)                                                                   |
| Amazon scraps secret AI recruiting tool that showed bias against women                                 | [https://www.euronews.com/business/2018/10/10/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women](https://www.euronews.com/business/2018/10/10/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women)                                                                                                   |
| DPD error caused chatbot to swear at customer                                                          | [https://www.bbc.co.uk/news/technology-68025677](https://www.bbc.co.uk/news/technology-68025677)                                                                                                                                                                                                                                             |
| ITutorGroup settles AI hiring lawsuit alleging age discrimination                                      | [https://www.verdict.co.uk/itutorgroup-settles-ai-hiring-lawsuit-alleging-age-discrimination/](https://www.verdict.co.uk/itutorgroup-settles-ai-hiring-lawsuit-alleging-age-discrimination/)                                                                                                                                                 |
| Generative AI: UNESCO study reveals alarming evidence of regressive gender stereotypes                 | [https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes](https://www.unesco.org/en/articles/generative-ai-unesco-study-reveals-alarming-evidence-regressive-gender-stereotypes)                                                                                               |
| Prankster tricks a GM chatbot into agreeing to sell him a $76,000 Chevy Tahoe for $1                   | [https://www.upworthy.com/prankster-tricks-a-gm-dealership-chatbot-to-sell-him-a-76000-chevy-tahoe-for-ex1](https://www.upworthy.com/prankster-tricks-a-gm-dealership-chatbot-to-sell-him-a-76000-chevy-tahoe-for-ex1)                                                                                                                       |
| L'hallucination du chatbot d'Air Canada r√©v√®le la responsabilit√© juridique des entreprises face √† l'IA | [https://www.lemondeinformatique.fr/actualites/lire-l-hallucination-du-chatbot-d-air-canada-revele-la-responsabilite-juridique-des-entreprises-face-a-l-ia-93025.html](https://www.lemondeinformatique.fr/actualites/lire-l-hallucination-du-chatbot-d-air-canada-revele-la-responsabilite-juridique-des-entreprises-face-a-l-ia-93025.html) |
| Google to fix AI picture bot after 'woke' criticism                                                    | [https://www.bbc.com/news/business-68364690](https://www.bbc.com/news/business-68364690)                                                                                                                                                                                                                                                     |
| 11 famous AI disasters                                                                                 | [https://www.cio.com/article/190888/5-famous-analytics-and-ai-disasters.html](https://www.cio.com/article/190888/5-famous-analytics-and-ai-disasters.html)                                                                                                                                                                                   |
