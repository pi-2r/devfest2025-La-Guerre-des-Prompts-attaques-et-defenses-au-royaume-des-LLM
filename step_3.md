# Introduction √† l'Attaque par Prompt Injection (LLM01)

## üéØ Objectifs de cette √©tape

## Sommaire 
- [2022, l‚Äôapparition des premi√®res pr√©occupations](#2022-lapparition-des-premieres-preoccupations)
- [Microsoft Tay : Chatbot corrompu par les utilisateurs](#microsoft-tay--chatbot-corrompu-par-les-utilisateurs)
- [D'autres exemples notables](#dautres-exemples-notables)
  - [AWS 2018](#aws-2018)
- [Ressources](#ressources)

## 2022, l‚Äôapparition des premi√®res pr√©occupations
Dans les mois qui ont suivi le lancement de ChatGPT en 2022, de s√©rieuses inqui√©tudes concernant la s√©curit√© et la 
confidentialit√© des donn√©es ont rapidement √©merg√©. Plusieurs incidents marquants, dont des fuites d‚Äôinformations 
personnelles et professionnelles, ont mis en √©vidence les risques associ√©s √† l‚Äôutilisation de cet outil. Face √† ces 
r√©v√©lations, certaines grandes entreprises comme Samsung ont pr√©f√©r√© bannir l‚Äôusage public de ChatGPT par leurs employ√©s,
tandis que des pays tels que l‚ÄôItalie ont impos√© des restrictions ou des interdictions temporaires, invoquant notamment
la non-conformit√© aux exigences r√©glementaires et aux principes de transparence.


Ces √©v√©nements ont raviv√© les souvenirs d‚Äô√©checs illustrant la vuln√©rabilit√© des intelligences artificielles, √† l‚Äôimage
du chatbot Tay de Microsoft, dont l‚Äôexp√©rience avait d√©j√† d√©montr√© √† quel point une IA pouvait √™tre d√©tourn√©e, mise en 
difficult√© par des probl√©matiques de s√©curit√© et de contr√¥le des contenus g√©n√©r√©s.


## Microsoft Tay : Chatbot corrompu par les utilisateurs

<a href="https://www.lemonde.fr/pixels/article/2016/03/24/a-peine-lancee-une-intelligence-artificielle-de-microsoft-derape-sur-twitter_4889661_4408996.html" target="_blank">
  <img src="https://img.lemde.fr/2016/03/24/0/0/516/220/1112/0/75/0/97393c4_9329-4mpfka.PNG" alt="tay " width="450" style="transition:0.3s;">
</a>

<a href="https://www.lemonde.fr/pixels/article/2016/03/24/a-peine-lancee-une-intelligence-artificielle-de-microsoft-derape-sur-twitter_4889661_4408996.html" target="_blank"><em>source: lemonde.fr</em></a>



En mars 2016, Microsoft a lanc√© Tay, un chatbot dot√© d‚Äôintelligence artificielle con√ßu pour dialoguer avec les 
utilisateurs sur Twitter et d‚Äôautres plateformes sociales. L‚Äôobjectif √©tait de cr√©er une IA capable d‚Äôapprendre et de 
s‚Äôadapter au langage des jeunes internautes en temps r√©el. 

Cependant, moins de 24 heures apr√®s sa mise en ligne, Tay a √©t√© la cible d‚Äôune campagne coordonn√©e d‚Äôutilisateurs 
malveillants qui ont exploit√© ses algorithmes d‚Äôapprentissage automatique pour le pousser √† g√©n√©rer des messages 
racistes, haineux et offensants. 

Cette exploitation des failles de s√©curit√© de Tay a provoqu√© un scandale retentissant. Face √† la quantit√© et √† la 
gravit√© des propos relay√©s, Microsoft a √©t√© contraint de d√©sactiver le chatbot imm√©diatement et a pr√©sent√© des excuses 
publiques, soulignant qu‚Äôils n‚Äôavaient pas anticip√© cette forme d‚Äôabus et qu‚Äôils prendraient √† l‚Äôavenir davantage de 
pr√©cautions dans le d√©ploiement de leurs IA.

## D'autres exemples notables
todo

## Ressources

| Information                                                                           | Lien                                                                                                                                                                                                                                                                             |
|---------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Arr√™tez de r√©v√©ler tous vos secrets √† ChatGPT, vous mettez votre entreprise en danger | [https://www.numerama.com/cyberguerre/1297046-arretez-de-reveler-tous-vos-secrets-a-chatgpt-vous-mettez-votre-entreprise-en-danger.html](https://www.numerama.com/cyberguerre/1297046-arretez-de-reveler-tous-vos-secrets-a-chatgpt-vous-mettez-votre-entreprise-en-danger.html) |
| Security Analysis of ChatGPT: Threats and Privacy Risks                               | [https://arxiv.org/html/2508.09426v1](https://arxiv.org/html/2508.09426v1)                                                                                                                                                                                                       |
| Microsoft shuts down AI chatbot after it turned into a Nazi                           | [https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/](https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/)                                                                                 |
| 5 Things That You Should Never Share With Chat GPT                                    | [https://agileblue.com/5-things-that-you-should-never-share-with-chat-gpt/](https://agileblue.com/5-things-that-you-should-never-share-with-chat-gpt/)                                                                                                                           |
| L'Italie bloque l'usage de l'intelligence artificielle ChatGPT                        | [https://www.france24.com/fr/%C3%A9co-tech/20230331-l-italie-bloque-l-usage-de-l-intelligence-artificielle-chatgpt](https://www.france24.com/fr/%C3%A9co-tech/20230331-l-italie-bloque-l-usage-de-l-intelligence-artificielle-chatgpt)                                           |
| Microsoft‚Äôs new AI-powered bot Tay answers your tweets and chats on GroupMe and Kik   | [https://techcrunch.com/2016/03/23/microsofts-new-ai-powered-bot-tay-answers-your-tweets-and-chats-on-groupme-and-kik/](https://techcrunch.com/2016/03/23/microsofts-new-ai-powered-bot-tay-answers-your-tweets-and-chats-on-groupme-and-kik/)                                   | 
| Microsoft Created a Twitter Bot to Learn from Users. It Quickly Became a Racist Jerk  | [https://www.nytimes.com/2016/03/25/technology/microsoft-created-a-twitter-bot-to-learn-from-users-it-quickly-became-a-racist-jerk.html](https://www.nytimes.com/2016/03/25/technology/microsoft-created-a-twitter-bot-to-learn-from-users-it-quickly-became-a-racist-jerk.html) |
| Microsoft shuts down AI chatbot after it turned into a Nazi                           | [https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/](https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/)                                                                                 |
